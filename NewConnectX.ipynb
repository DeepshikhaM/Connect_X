{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22469b65-b12c-4cab-b184-6570aa796e34",
   "metadata": {},
   "source": [
    "# Done by Deepshikha Mahato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6c3db5-7564-47db-a53b-cd8c70f0af48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle-environments in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (1.16.9)\n",
      "Requirement already satisfied: Chessnut>=0.3.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (0.4.1)\n",
      "Requirement already satisfied: Flask>=1.1.2 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (3.1.0)\n",
      "Requirement already satisfied: gymnasium==0.29.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (0.29.0)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (4.23.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (2.0.2)\n",
      "Requirement already satisfied: pettingzoo==1.24.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (1.24.0)\n",
      "Requirement already satisfied: requests>=2.25.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.11.2 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (1.13.1)\n",
      "Requirement already satisfied: shimmy>=1.2.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (1.3.0)\n",
      "Requirement already satisfied: stable-baselines3==2.1.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (2.1.0)\n",
      "Requirement already satisfied: transformers>=4.33.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from kaggle-environments) (4.47.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from gymnasium==0.29.0->kaggle-environments) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from gymnasium==0.29.0->kaggle-environments) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from gymnasium==0.29.0->kaggle-environments) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from gymnasium==0.29.0->kaggle-environments) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.13 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from stable-baselines3==2.1.0->kaggle-environments) (2.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from stable-baselines3==2.1.0->kaggle-environments) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from stable-baselines3==2.1.0->kaggle-environments) (3.9.3)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from Flask>=1.1.2->kaggle-environments) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from Flask>=1.1.2->kaggle-environments) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from Flask>=1.1.2->kaggle-environments) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from Flask>=1.1.2->kaggle-environments) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from Flask>=1.1.2->kaggle-environments) (1.9.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from jsonschema>=3.0.1->kaggle-environments) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from jsonschema>=3.0.1->kaggle-environments) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from jsonschema>=3.0.1->kaggle-environments) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from jsonschema>=3.0.1->kaggle-environments) (0.22.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from requests>=2.25.1->kaggle-environments) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from requests>=2.25.1->kaggle-environments) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from requests>=2.25.1->kaggle-environments) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from requests>=2.25.1->kaggle-environments) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (0.26.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from transformers>=4.33.1->kaggle-environments) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from click>=8.1.3->Flask>=1.1.2->kaggle-environments) (0.4.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.33.1->kaggle-environments) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium==0.29.0->kaggle-environments) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from Jinja2>=3.1.2->Flask>=1.1.2->kaggle-environments) (3.0.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from torch>=1.13->stable-baselines3==2.1.0->kaggle-environments) (3.2.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from torch>=1.13->stable-baselines3==2.1.0->kaggle-environments) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3==2.1.0->kaggle-environments) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from matplotlib->stable-baselines3==2.1.0->kaggle-environments) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from pandas->stable-baselines3==2.1.0->kaggle-environments) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from pandas->stable-baselines3==2.1.0->kaggle-environments) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\deepshikha\\downloads\\connectx_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.1.0->kaggle-environments) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install kaggle-environments\n",
    "!pip install kaggle-environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dee3e0f7-3c7b-4151-a535-ef345bf19d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the ConnectX environment\n",
    "env = make(\"connectx\", debug=True)\n",
    "# Render the ConnectX board in the console (console output displayed in the notebook)\n",
    "print(env.render(mode=\"ansi\"))\n",
    "\n",
    "# Define observation and action spaces\n",
    "BOARD_SIZE = env.configuration.rows * env.configuration.columns\n",
    "ACTION_SPACE = env.configuration.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ccbde1a-03fb-4eea-8449-3a4201a4b400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/chess.js@0.12.0/chess.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;8e02b08c-b6be-11ef-8ce1-847b57b245e8&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;columns&quot;: 7,\n",
       "      &quot;rows&quot;: 6,\n",
       "      &quot;inarow&quot;: 4,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 8,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 9,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 10,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 11,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 12,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              2,\n",
       "              1,\n",
       "              1\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 4,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      -1,\n",
       "      1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 5.7e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000482,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 7.4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.5e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.6e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;width&quot;: 500,\n",
       "  &quot;height&quot;: 450\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"500\" height=\"450\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a random agent for baseline comparison\n",
    "def random_agent(observation, configuration):\n",
    "    from random import choice\n",
    "    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n",
    "\n",
    "# Test the DM agent\n",
    "env.reset()\n",
    "env.run([random_agent, \"random\"])\n",
    "env.render(mode=\"ipython\", width=500, height=450)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1ad83-6110-4d05-a1a1-34d0953e9087",
   "metadata": {},
   "source": [
    "### Importing Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "196ee8ef-d358-4c02-a593-a0732d155846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0fbc61-73b9-4013-98ea-a3f8fb6eb066",
   "metadata": {},
   "source": [
    "### 1. Firstly trying the DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cf352d6-8f7b-4628-b8d9-1a5e050b347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed5945f-d881-4c32-8aa6-9b8b5e16cbfd",
   "metadata": {},
   "source": [
    "### Defining the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5dd8cc-9f72-4a25-83f8-c32c38e0caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 64\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += self.gamma * torch.max(self.target_model(next_state_tensor)).item()\n",
    "\n",
    "            q_values = self.model(state_tensor)\n",
    "            target_f = q_values.clone().detach()\n",
    "            target_f[action] = target\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = nn.MSELoss()(q_values, target_f)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fef7631-f264-4b7c-917f-19b83c8535f0",
   "metadata": {},
   "source": [
    "#### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6afbe013-0aca-4d88-81fe-c979507c035b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Reward: -1, Epsilon: 1.00\n",
      "Episode 2/100, Reward: 1, Epsilon: 1.00\n",
      "Episode 3/100, Reward: 1, Epsilon: 1.00\n",
      "Episode 4/100, Reward: 1, Epsilon: 1.00\n",
      "Episode 5/100, Reward: 0, Epsilon: 1.00\n",
      "Episode 6/100, Reward: -1, Epsilon: 0.97\n",
      "Episode 7/100, Reward: 1, Epsilon: 0.92\n",
      "Episode 8/100, Reward: 1, Epsilon: 0.88\n",
      "Episode 9/100, Reward: -1, Epsilon: 0.83\n",
      "Episode 10/100, Reward: 1, Epsilon: 0.79\n",
      "Episode 11/100, Reward: 1, Epsilon: 0.75\n",
      "Episode 12/100, Reward: 0, Epsilon: 0.70\n",
      "Episode 13/100, Reward: 0, Epsilon: 0.67\n",
      "Episode 14/100, Reward: 1, Epsilon: 0.65\n",
      "Episode 15/100, Reward: -1, Epsilon: 0.62\n",
      "Episode 16/100, Reward: -1, Epsilon: 0.58\n",
      "Episode 17/100, Reward: -1, Epsilon: 0.57\n",
      "Episode 18/100, Reward: 1, Epsilon: 0.53\n",
      "Episode 19/100, Reward: 1, Epsilon: 0.50\n",
      "Episode 20/100, Reward: 1, Epsilon: 0.47\n",
      "Episode 21/100, Reward: 0, Epsilon: 0.44\n",
      "Episode 22/100, Reward: -1, Epsilon: 0.42\n",
      "Episode 23/100, Reward: 0, Epsilon: 0.40\n",
      "Episode 24/100, Reward: 1, Epsilon: 0.37\n",
      "Episode 25/100, Reward: 1, Epsilon: 0.36\n",
      "Episode 26/100, Reward: -1, Epsilon: 0.35\n",
      "Episode 27/100, Reward: -1, Epsilon: 0.33\n",
      "Episode 28/100, Reward: 0, Epsilon: 0.31\n",
      "Episode 29/100, Reward: 1, Epsilon: 0.29\n",
      "Episode 30/100, Reward: -1, Epsilon: 0.28\n",
      "Episode 31/100, Reward: -1, Epsilon: 0.27\n",
      "Episode 32/100, Reward: 1, Epsilon: 0.26\n",
      "Episode 33/100, Reward: 1, Epsilon: 0.26\n",
      "Episode 34/100, Reward: -1, Epsilon: 0.25\n",
      "Episode 35/100, Reward: 1, Epsilon: 0.24\n",
      "Episode 36/100, Reward: 0, Epsilon: 0.22\n",
      "Episode 37/100, Reward: -1, Epsilon: 0.21\n",
      "Episode 38/100, Reward: 0, Epsilon: 0.20\n",
      "Episode 39/100, Reward: -1, Epsilon: 0.19\n",
      "Episode 40/100, Reward: 1, Epsilon: 0.18\n",
      "Episode 41/100, Reward: 1, Epsilon: 0.17\n",
      "Episode 42/100, Reward: 1, Epsilon: 0.16\n",
      "Episode 43/100, Reward: 1, Epsilon: 0.15\n",
      "Episode 44/100, Reward: 1, Epsilon: 0.15\n",
      "Episode 45/100, Reward: 1, Epsilon: 0.14\n",
      "Episode 46/100, Reward: 1, Epsilon: 0.14\n",
      "Episode 47/100, Reward: 0, Epsilon: 0.13\n",
      "Episode 48/100, Reward: -1, Epsilon: 0.13\n",
      "Episode 49/100, Reward: 1, Epsilon: 0.12\n",
      "Episode 50/100, Reward: -1, Epsilon: 0.11\n",
      "Episode 51/100, Reward: 1, Epsilon: 0.11\n",
      "Episode 52/100, Reward: 1, Epsilon: 0.11\n",
      "Episode 53/100, Reward: 1, Epsilon: 0.10\n",
      "Episode 54/100, Reward: 1, Epsilon: 0.10\n",
      "Episode 55/100, Reward: -1, Epsilon: 0.09\n",
      "Episode 56/100, Reward: 1, Epsilon: 0.09\n",
      "Episode 57/100, Reward: 1, Epsilon: 0.08\n",
      "Episode 58/100, Reward: 1, Epsilon: 0.08\n",
      "Episode 59/100, Reward: -1, Epsilon: 0.08\n",
      "Episode 60/100, Reward: 1, Epsilon: 0.07\n",
      "Episode 61/100, Reward: 1, Epsilon: 0.07\n",
      "Episode 62/100, Reward: 1, Epsilon: 0.07\n",
      "Episode 63/100, Reward: 1, Epsilon: 0.06\n",
      "Episode 64/100, Reward: 1, Epsilon: 0.06\n",
      "Episode 65/100, Reward: 1, Epsilon: 0.06\n",
      "Episode 66/100, Reward: -1, Epsilon: 0.05\n",
      "Episode 67/100, Reward: -1, Epsilon: 0.05\n",
      "Episode 68/100, Reward: 1, Epsilon: 0.05\n",
      "Episode 69/100, Reward: -1, Epsilon: 0.05\n",
      "Episode 70/100, Reward: 1, Epsilon: 0.05\n",
      "Episode 71/100, Reward: -1, Epsilon: 0.04\n",
      "Episode 72/100, Reward: 1, Epsilon: 0.04\n",
      "Episode 73/100, Reward: 1, Epsilon: 0.04\n",
      "Episode 74/100, Reward: -1, Epsilon: 0.04\n",
      "Episode 75/100, Reward: 0, Epsilon: 0.04\n",
      "Episode 76/100, Reward: 1, Epsilon: 0.04\n",
      "Episode 77/100, Reward: 1, Epsilon: 0.03\n",
      "Episode 78/100, Reward: 1, Epsilon: 0.03\n",
      "Episode 79/100, Reward: 1, Epsilon: 0.03\n",
      "Episode 80/100, Reward: 1, Epsilon: 0.03\n",
      "Episode 81/100, Reward: 1, Epsilon: 0.03\n",
      "Episode 82/100, Reward: -1, Epsilon: 0.03\n",
      "Episode 83/100, Reward: -1, Epsilon: 0.02\n",
      "Episode 84/100, Reward: 1, Epsilon: 0.02\n",
      "Episode 85/100, Reward: 1, Epsilon: 0.02\n",
      "Episode 86/100, Reward: 1, Epsilon: 0.02\n",
      "Episode 87/100, Reward: 1, Epsilon: 0.02\n",
      "Episode 88/100, Reward: 1, Epsilon: 0.02\n",
      "Episode 89/100, Reward: 0, Epsilon: 0.02\n",
      "Episode 90/100, Reward: -1, Epsilon: 0.02\n",
      "Episode 91/100, Reward: 1, Epsilon: 0.02\n",
      "Episode 92/100, Reward: 1, Epsilon: 0.02\n",
      "Episode 93/100, Reward: 0, Epsilon: 0.01\n",
      "Episode 94/100, Reward: 1, Epsilon: 0.01\n",
      "Episode 95/100, Reward: -1, Epsilon: 0.01\n",
      "Episode 96/100, Reward: 1, Epsilon: 0.01\n",
      "Episode 97/100, Reward: 0, Epsilon: 0.01\n",
      "Episode 98/100, Reward: 1, Epsilon: 0.01\n",
      "Episode 99/100, Reward: -1, Epsilon: 0.01\n",
      "Episode 100/100, Reward: 1, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "env = make(\"connectx\")\n",
    "state_size = env.configuration.rows * env.configuration.columns\n",
    "action_size = env.configuration.columns\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "EPISODES = 100\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    trainer = env.train([None, \"random\"])\n",
    "    state = trainer.reset()\n",
    "    state = np.array(state[\"board\"]).reshape(-1)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_step = trainer.step(action)\n",
    "        next_state = np.array(next_step[0][\"board\"]).reshape(-1)\n",
    "        reward = next_step[1] if next_step[1] is not None else 0\n",
    "        done = next_step[2]\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{EPISODES}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df515d0-2554-4ad1-a25d-f64cdd2a87f0",
   "metadata": {},
   "source": [
    "#### Evaluating the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bafbcf-af2e-4823-bc87-51e267acc238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent vs Random Agent: 0.2\n"
     ]
    }
   ],
   "source": [
    "def evaluate(env_name, agent, opponent, num_episodes=10):\n",
    "    \n",
    "    env = make(env_name)\n",
    "    rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Extract the board for the current agent\n",
    "        if isinstance(state[0], dict) and \"board\" in state[0][\"observation\"]:\n",
    "            state = np.array(state[0][\"observation\"][\"board\"]).reshape(-1)  # Extract board\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected observation format in evaluation.\")\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Agent selects an action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Opponent logic\n",
    "            if callable(opponent):\n",
    "                opponent_action = opponent(state)\n",
    "            else:\n",
    "                opponent_action = random.choice([c for c in range(env.configuration.columns) if state[c] == 0])\n",
    "            \n",
    "            # Perform both agent and opponent actions\n",
    "            next_step = env.step([action, opponent_action])\n",
    "            done = env.done\n",
    "            \n",
    "            # Update state and reward\n",
    "            if next_step[0][\"reward\"] is not None:\n",
    "                total_reward += next_step[0][\"reward\"]\n",
    "            \n",
    "            # Extract the next board state for the agent\n",
    "            if isinstance(next_step[0], dict) and \"board\" in next_step[0][\"observation\"]:\n",
    "                state = np.array(next_step[0][\"observation\"][\"board\"]).reshape(-1)\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected observation format in evaluation step.\")\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "# Use the updated function\n",
    "print(\"DQN Agent vs Random Agent:\", evaluate(\"connectx\", agent, \"random\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35560c-bd3f-48a7-b7a2-23c3b9eb7560",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "1. **Positive Reward:** A reward of 0.2 suggests that the agent is slightly better than a random agent but not significantly. This result is expected for an early iteration or a simple DQN implementation.\n",
    "\n",
    "2. **Learning Rate and Exploration:** The agent's epsilon () decays over episodes, but if the training data is not diverse or the reward structure is not incentivizing optimal moves, the agent might struggle to consistently outperform random strategies.\n",
    "\n",
    "3. **Evaluation Sample Size:** The number of evaluation episodes (num_episodes=10) is relatively small for a robust assessment. A larger sample size (e.g., 100 episodes) could provide more reliable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7c433-70cc-428d-aaa2-b6aae782bcdf",
   "metadata": {},
   "source": [
    "## Secondly trying the Double DQN Implementation:\n",
    "- Double DQN (DDQN) by extending your current DQN architecture and Double DQN improves upon the vanilla DQN by reducing overestimation bias during value estimation which is done by separating the selection of actions and the evaluation of action values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56fe1928-3800-4a1b-bc76-722f547ea670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# Define the Double DQN Agent\n",
    "class DoubleDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Neural networks\n",
    "        self.model = self._build_model().to(self.device)\n",
    "        self.target_model = self._build_model().to(self.device)\n",
    "        self.update_target_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def _build_model(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.state_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, self.action_size)\n",
    "        )\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state_tensor)\n",
    "        return torch.argmax(act_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Double DQN Target Calculation\n",
    "            with torch.no_grad():\n",
    "                next_action = torch.argmax(self.model(next_state_tensor)).item()\n",
    "                target_q_value = self.target_model(next_state_tensor)[next_action]\n",
    "                target = reward + (self.gamma * target_q_value * (1 - done))\n",
    "\n",
    "            q_values = self.model(state_tensor)\n",
    "            target_q_values = q_values.clone().detach()\n",
    "            target_q_values[action] = target  # Update the target for the chosen action\n",
    "\n",
    "            # Compute the loss and backpropagate\n",
    "            self.model.zero_grad()\n",
    "            loss = nn.MSELoss()(q_values, target_q_values)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638c148-fb51-4e28-ad1c-238b4951359b",
   "metadata": {},
   "source": [
    "#### Training the Double DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cffa57f-8440-4211-8e96-8b1376b0562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Reward: 1, Epsilon: 1.00\n",
      "Episode 2/200, Reward: -1, Epsilon: 1.00\n",
      "Episode 3/200, Reward: -1, Epsilon: 1.00\n",
      "Episode 4/200, Reward: -1, Epsilon: 0.93\n",
      "Episode 5/200, Reward: 1, Epsilon: 0.85\n",
      "Episode 6/200, Reward: 1, Epsilon: 0.80\n",
      "Episode 7/200, Reward: -1, Epsilon: 0.67\n",
      "Episode 8/200, Reward: 1, Epsilon: 0.56\n",
      "Episode 9/200, Reward: 1, Epsilon: 0.46\n",
      "Episode 10/200, Reward: 1, Epsilon: 0.43\n",
      "Episode 11/200, Reward: 1, Epsilon: 0.40\n",
      "Episode 12/200, Reward: -1, Epsilon: 0.34\n",
      "Episode 13/200, Reward: 1, Epsilon: 0.31\n",
      "Episode 14/200, Reward: -1, Epsilon: 0.29\n",
      "Episode 15/200, Reward: 1, Epsilon: 0.27\n",
      "Episode 16/200, Reward: 1, Epsilon: 0.26\n",
      "Episode 17/200, Reward: 1, Epsilon: 0.24\n",
      "Episode 18/200, Reward: -1, Epsilon: 0.21\n",
      "Episode 19/200, Reward: 1, Epsilon: 0.20\n",
      "Episode 20/200, Reward: 1, Epsilon: 0.17\n",
      "Episode 21/200, Reward: 1, Epsilon: 0.17\n",
      "Episode 22/200, Reward: 1, Epsilon: 0.15\n",
      "Episode 23/200, Reward: 1, Epsilon: 0.14\n",
      "Episode 24/200, Reward: -1, Epsilon: 0.13\n",
      "Episode 25/200, Reward: 1, Epsilon: 0.12\n",
      "Episode 26/200, Reward: -1, Epsilon: 0.11\n",
      "Episode 27/200, Reward: 1, Epsilon: 0.10\n",
      "Episode 28/200, Reward: 1, Epsilon: 0.09\n",
      "Episode 29/200, Reward: 1, Epsilon: 0.08\n",
      "Episode 30/200, Reward: 1, Epsilon: 0.07\n",
      "Episode 31/200, Reward: 1, Epsilon: 0.07\n",
      "Episode 32/200, Reward: 1, Epsilon: 0.06\n",
      "Episode 33/200, Reward: -1, Epsilon: 0.06\n",
      "Episode 34/200, Reward: 1, Epsilon: 0.05\n",
      "Episode 35/200, Reward: 1, Epsilon: 0.05\n",
      "Episode 36/200, Reward: -1, Epsilon: 0.04\n",
      "Episode 37/200, Reward: -1, Epsilon: 0.03\n",
      "Episode 38/200, Reward: -1, Epsilon: 0.03\n",
      "Episode 39/200, Reward: 1, Epsilon: 0.03\n",
      "Episode 40/200, Reward: 1, Epsilon: 0.03\n",
      "Episode 41/200, Reward: 1, Epsilon: 0.03\n",
      "Episode 42/200, Reward: -1, Epsilon: 0.02\n",
      "Episode 43/200, Reward: 1, Epsilon: 0.02\n",
      "Episode 44/200, Reward: 1, Epsilon: 0.02\n",
      "Episode 45/200, Reward: 1, Epsilon: 0.02\n",
      "Episode 46/200, Reward: -1, Epsilon: 0.02\n",
      "Episode 47/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 48/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 49/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 50/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 51/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 52/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 53/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 54/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 55/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 56/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 57/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 58/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 59/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 60/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 61/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 62/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 63/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 64/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 65/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 66/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 67/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 68/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 69/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 70/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 71/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 72/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 73/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 74/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 75/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 76/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 77/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 78/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 79/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 80/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 81/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 82/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 83/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 84/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 85/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 86/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 87/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 88/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 89/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 90/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 91/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 92/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 93/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 94/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 95/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 96/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 97/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 98/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 99/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 100/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 101/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 102/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 103/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 104/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 105/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 106/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 107/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 108/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 109/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 110/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 111/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 112/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 113/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 114/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 115/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 116/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 117/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 118/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 119/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 120/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 121/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 122/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 123/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 124/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 125/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 126/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 127/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 128/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 129/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 130/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 131/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 132/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 133/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 134/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 135/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 136/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 137/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 138/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 139/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 140/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 141/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 142/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 143/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 144/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 145/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 146/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 147/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 148/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 149/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 150/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 151/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 152/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 153/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 154/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 155/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 156/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 157/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 158/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 159/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 160/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 161/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 162/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 163/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 164/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 165/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 166/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 167/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 168/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 169/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 170/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 171/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 172/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 173/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 174/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 175/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 176/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 177/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 178/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 179/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 180/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 181/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 182/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 183/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 184/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 185/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 186/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 187/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 188/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 189/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 190/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 191/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 192/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 193/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 194/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 195/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 196/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 197/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 198/200, Reward: -1, Epsilon: 0.01\n",
      "Episode 199/200, Reward: 1, Epsilon: 0.01\n",
      "Episode 200/200, Reward: 1, Epsilon: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Set up the environment\n",
    "env = make(\"connectx\")\n",
    "state_size = env.configuration.rows * env.configuration.columns\n",
    "action_size = env.configuration.columns\n",
    "\n",
    "# Initialize Double DQN Agent\n",
    "agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "# Training parameters\n",
    "EPISODES = 200\n",
    "TARGET_UPDATE = 10\n",
    "BATCH_SIZE = 64\n",
    "rewards = []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.array(state[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Get valid actions (columns that are not full)\n",
    "        valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "        \n",
    "        # Choose an action\n",
    "        action = agent.act(state)\n",
    "        if action not in valid_actions:\n",
    "            action = random.choice(valid_actions)\n",
    "\n",
    "        # Opponent action\n",
    "        opponent_action = random.choice(valid_actions)\n",
    "\n",
    "        # Step in the environment\n",
    "        next_step = env.step([action, opponent_action])\n",
    "        done = env.done\n",
    "        reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "\n",
    "        next_state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay(BATCH_SIZE)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{EPISODES}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a5c2e22-abeb-48be-9977-8583b2a26657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Double DQN Agent vs Random Agent: 0.58\n"
     ]
    }
   ],
   "source": [
    "def evaluate(env_name, agent, opponent, num_episodes=100):\n",
    "    env = make(env_name)\n",
    "    rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.array(state[0]['observation'][\"board\"]).reshape(-1)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "            if action not in valid_actions:\n",
    "                action = random.choice(valid_actions)\n",
    "\n",
    "            opponent_action = random.choice(valid_actions)\n",
    "            next_step = env.step([action, opponent_action])\n",
    "            done = env.done\n",
    "            reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "            state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "print(\"Double DQN Agent vs Random Agent:\", evaluate(\"connectx\", agent, \"random\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd14877-fc2a-4c94-b4b8-34e581ed8242",
   "metadata": {},
   "source": [
    "- **Double DQN Agent vs Random Agent:** 0.58 indicates that the Double DQN agent wins 58% of the games on average against a random opponent, this is a solid improvement compared to standard DQN or other baseline agents (e.g., a Random Agent).\n",
    "\n",
    "- **Positive Learning Trend:** The Double DQN agent is learning effectively and showing better performance compared to a random strategy, a win rate above 50% is encouraging, especially against a random opponent.\n",
    "\n",
    "- **Room for Improvement:** While 58% is a decent win rate, there's still potential to enhance the agent further. It hasn't yet fully mastered the game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97bbf4f-db3b-401f-9641-a6c16e82b486",
   "metadata": {},
   "source": [
    "\n",
    "## Improvisation on DoubleDQNA Model\n",
    "\n",
    "#### 1. Fine-Tuning Hyperparameters\n",
    "- We'll adjust the following hyperparameters:\n",
    "\n",
    "- Learning Rate: Reduce it slightly to improve stability during training.\n",
    "- Batch Size: Increase it to 128 for more stable updates.\n",
    "- Gamma (Discount Factor): Adjust it closer to 1 to prioritize long-term rewards.\n",
    "\n",
    "#### 2. Incorporate Reward Shaping:\n",
    "   \n",
    "- We will Add a small positive reward for blocking the opponent's potential winning move & Add intermediate rewards for actions creating sequences of two or three checkers.\n",
    "\n",
    "- Train Against Mixed Opponents\n",
    "\n",
    "#### 3. To make the agent more robust:\n",
    "\n",
    "- Alternate between training against Random Agent and Negamax Agent.\n",
    " \n",
    "#### 4. Evaluate Against Negamax Agent\n",
    "- After training, evaluate the Double DQN agent's performance against both: Random Agent & Negamax Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e9ec9-827a-4116-aa2a-30e56f1d087b",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from kaggle_environments import make\n",
    "\n",
    "# Environment setup\n",
    "env = make(\"connectx\")\n",
    "state_size = env.configuration.rows * env.configuration.columns\n",
    "action_size = env.configuration.columns\n",
    "\n",
    "# Initialize the enhanced Double DQN Agent\n",
    "agent = DoubleDQNAgent(state_size, action_size)\n",
    "\n",
    "# Fine-tuned hyperparameters\n",
    "EPISODES = 200\n",
    "TARGET_UPDATE = 10\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.005\n",
    "GAMMA = 0.99\n",
    "agent.learning_rate = LEARNING_RATE\n",
    "agent.gamma = GAMMA\n",
    "\n",
    "rewards = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.array(state[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Get valid actions\n",
    "        valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "        \n",
    "        # Choose action\n",
    "        action = agent.act(state)\n",
    "        if action not in valid_actions:\n",
    "            action = random.choice(valid_actions)\n",
    "        \n",
    "        # Opponent action (random or negamax alternation)\n",
    "        opponent_action = random.choice(valid_actions) if episode % 2 == 0 else None  # Replace with Negamax logic if available\n",
    "        \n",
    "        # Step in the environment\n",
    "        next_step = env.step([action, opponent_action])\n",
    "        done = env.done\n",
    "        reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "\n",
    "        # Reward shaping\n",
    "        if reward == 0 and not done:\n",
    "            reward += 0.1  # Reward for intermediate actions\n",
    "        if reward < 0:\n",
    "            reward -= 0.5  # Penalize losing moves\n",
    "        \n",
    "        next_state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)\n",
    "\n",
    "        # Update agent\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        agent.replay(BATCH_SIZE)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{EPISODES}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(env_name, agent, opponent, num_episodes=100):\n",
    "    env = make(env_name)\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.array(state[0]['observation'][\"board\"]).reshape(-1)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "            if action not in valid_actions:\n",
    "                action = random.choice(valid_actions)\n",
    "            \n",
    "            opponent_action = random.choice(valid_actions) if opponent == \"random\" else None  # Replace with Negamax logic if needed\n",
    "            next_step = env.step([action, opponent_action])\n",
    "            done = env.done\n",
    "            reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "\n",
    "            state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    return np.mean(total_rewards)\n",
    "\n",
    "# Evaluate against Random and Negamax Agents\n",
    "print(\"Double DQN Agent vs Random Agent:\", evaluate(\"connectx\", agent, \"random\"))\n",
    "print(\"Double DQN Agent vs Negamax Agent:\", evaluate(\"connectx\", agent, \"negamax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f552ef-f212-4f39-9da8-df38f3152e7a",
   "metadata": {},
   "source": [
    "## Performance Analysis:\n",
    "### 1. Random Agent Evaluation:\n",
    "\n",
    "- Average Reward: 0.44: The agent is performing positively against the random agent, which indicates learning progress. However, it can still improve to consistently outperform random actions.\n",
    "\n",
    "### 2. Negamax Agent Evaluation:\n",
    "\n",
    "- Average Reward: -0.98: The agent struggles against the Negamax agent, which is a more sophisticated opponent using deterministic strategies. This reflects the need for further enhancements in training or model architecture to address the Negamax's deterministic approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907407fa-c43f-41ea-a5bb-c7e835a3a1e7",
   "metadata": {},
   "source": [
    "# PPO Implementation \n",
    "1. Policy-Based Optimization: PPO directly optimizes the policy, unlike Q-learning-based methods (DQN, Double DQN) that learn a value function. This approach makes PPO more suitable for environments with high-dimensional or continuous action spaces, though it also works well with discrete action spaces like ConnectX.\n",
    "\n",
    "2. Stable Updates: PPO ensures stable policy updates using a clipped objective function. This avoids large policy changes that might destabilize learning. By limiting how much the policy can deviate in a single update, PPO strikes a balance between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43fab2-c2ce-4f08-89a9-36a4f230a6c2",
   "metadata": {},
   "source": [
    "#### PPO Network & Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b91d69f3-ae8f-4f85-9681-9a7e34a45b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 1\n",
      "Episode 2/100, Total Reward: -1\n",
      "Episode 3/100, Total Reward: 1\n",
      "Episode 4/100, Total Reward: 1\n",
      "Episode 5/100, Total Reward: -1\n",
      "Episode 6/100, Total Reward: 1\n",
      "Episode 7/100, Total Reward: -1\n",
      "Episode 8/100, Total Reward: 1\n",
      "Episode 9/100, Total Reward: 1\n",
      "Episode 10/100, Total Reward: 1\n",
      "Episode 11/100, Total Reward: -1\n",
      "Episode 12/100, Total Reward: -1\n",
      "Episode 13/100, Total Reward: -1\n",
      "Episode 14/100, Total Reward: 1\n",
      "Episode 15/100, Total Reward: 1\n",
      "Episode 16/100, Total Reward: 1\n",
      "Episode 17/100, Total Reward: -1\n",
      "Episode 18/100, Total Reward: -1\n",
      "Episode 19/100, Total Reward: -1\n",
      "Episode 20/100, Total Reward: 1\n",
      "Episode 21/100, Total Reward: 1\n",
      "Episode 22/100, Total Reward: -1\n",
      "Episode 23/100, Total Reward: 1\n",
      "Episode 24/100, Total Reward: -1\n",
      "Episode 25/100, Total Reward: 1\n",
      "Episode 26/100, Total Reward: 1\n",
      "Episode 27/100, Total Reward: 1\n",
      "Episode 28/100, Total Reward: -1\n",
      "Episode 29/100, Total Reward: 1\n",
      "Episode 30/100, Total Reward: -1\n",
      "Episode 31/100, Total Reward: -1\n",
      "Episode 32/100, Total Reward: 1\n",
      "Episode 33/100, Total Reward: 1\n",
      "Episode 34/100, Total Reward: -1\n",
      "Episode 35/100, Total Reward: -1\n",
      "Episode 36/100, Total Reward: 1\n",
      "Episode 37/100, Total Reward: -1\n",
      "Episode 38/100, Total Reward: 1\n",
      "Episode 39/100, Total Reward: 1\n",
      "Episode 40/100, Total Reward: -1\n",
      "Episode 41/100, Total Reward: 1\n",
      "Episode 42/100, Total Reward: 1\n",
      "Episode 43/100, Total Reward: 1\n",
      "Episode 44/100, Total Reward: 1\n",
      "Episode 45/100, Total Reward: 1\n",
      "Episode 46/100, Total Reward: 1\n",
      "Episode 47/100, Total Reward: 1\n",
      "Episode 48/100, Total Reward: -1\n",
      "Episode 49/100, Total Reward: -1\n",
      "Episode 50/100, Total Reward: -1\n",
      "Episode 51/100, Total Reward: 1\n",
      "Episode 52/100, Total Reward: -1\n",
      "Episode 53/100, Total Reward: 1\n",
      "Episode 54/100, Total Reward: 1\n",
      "Episode 55/100, Total Reward: -1\n",
      "Episode 56/100, Total Reward: 1\n",
      "Episode 57/100, Total Reward: 1\n",
      "Episode 58/100, Total Reward: -1\n",
      "Episode 59/100, Total Reward: -1\n",
      "Episode 60/100, Total Reward: -1\n",
      "Episode 61/100, Total Reward: 1\n",
      "Episode 62/100, Total Reward: 1\n",
      "Episode 63/100, Total Reward: -1\n",
      "Episode 64/100, Total Reward: -1\n",
      "Episode 65/100, Total Reward: 1\n",
      "Episode 66/100, Total Reward: 1\n",
      "Episode 67/100, Total Reward: 1\n",
      "Episode 68/100, Total Reward: -1\n",
      "Episode 69/100, Total Reward: 1\n",
      "Episode 70/100, Total Reward: 1\n",
      "Episode 71/100, Total Reward: -1\n",
      "Episode 72/100, Total Reward: 1\n",
      "Episode 73/100, Total Reward: 1\n",
      "Episode 74/100, Total Reward: -1\n",
      "Episode 75/100, Total Reward: 1\n",
      "Episode 76/100, Total Reward: -1\n",
      "Episode 77/100, Total Reward: -1\n",
      "Episode 78/100, Total Reward: 1\n",
      "Episode 79/100, Total Reward: -1\n",
      "Episode 80/100, Total Reward: 1\n",
      "Episode 81/100, Total Reward: 1\n",
      "Episode 82/100, Total Reward: 1\n",
      "Episode 83/100, Total Reward: 1\n",
      "Episode 84/100, Total Reward: -1\n",
      "Episode 85/100, Total Reward: -1\n",
      "Episode 86/100, Total Reward: 1\n",
      "Episode 87/100, Total Reward: 1\n",
      "Episode 88/100, Total Reward: -1\n",
      "Episode 89/100, Total Reward: 1\n",
      "Episode 90/100, Total Reward: -1\n",
      "Episode 91/100, Total Reward: 1\n",
      "Episode 92/100, Total Reward: -1\n",
      "Episode 93/100, Total Reward: -1\n",
      "Episode 94/100, Total Reward: 1\n",
      "Episode 95/100, Total Reward: -1\n",
      "Episode 96/100, Total Reward: 1\n",
      "Episode 97/100, Total Reward: -1\n",
      "Episode 98/100, Total Reward: 1\n",
      "Episode 99/100, Total Reward: -1\n",
      "Episode 100/100, Total Reward: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from kaggle_environments import make\n",
    "\n",
    "# Set up the environment\n",
    "env = make(\"connectx\", debug=True)\n",
    "state_size = env.configuration.rows * env.configuration.columns\n",
    "action_size = env.configuration.columns\n",
    "\n",
    "# Define the PPO Actor-Critic Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.actor = nn.Linear(128, action_size)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_size, action_size, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.model = PPOActorCritic(state_size, action_size).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        logits, _ = self.model(state)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probabilities, 1).item()\n",
    "        return action, probabilities.detach().cpu().numpy()  # Return full probabilities\n",
    "\n",
    "    def evaluate_action(self, states, actions):\n",
    "        logits, values = self.model(states)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        action_probs = probabilities.gather(1, actions.unsqueeze(-1)).squeeze(-1)  # Select probs of chosen actions\n",
    "        return action_probs, values\n",
    "\n",
    "    def train(self, memory, batch_size):\n",
    "        states, actions, rewards, next_states, dones, old_probs = zip(*memory)\n",
    "        states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        old_probs = torch.tensor(old_probs, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Normalize rewards\n",
    "        advantages = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "\n",
    "        for _ in range(4):  # Multiple epochs\n",
    "            new_probs, _ = self.evaluate_action(states, actions)\n",
    "            ratio = new_probs / old_probs  # Calculate ratio\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "# Training PPO Agent\n",
    "EPISODES = 100\n",
    "BATCH_SIZE = 64\n",
    "rewards = []\n",
    "memory = []\n",
    "\n",
    "# Initialize PPO Agent\n",
    "agent = PPOAgent(state_size, action_size)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.array(state[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        # Get valid actions\n",
    "        valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "        \n",
    "        # Choose an action\n",
    "        action, probs = agent.act(state)\n",
    "        if action not in valid_actions:\n",
    "            action = random.choice(valid_actions)\n",
    "\n",
    "        # Opponent action\n",
    "        opponent_action = random.choice(valid_actions)\n",
    "\n",
    "        # Step in the environment\n",
    "        next_step = env.step([action, opponent_action])\n",
    "        done = env.done\n",
    "        reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "        next_state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "\n",
    "        # Store the experience in memory\n",
    "        memory.append((state, action, reward, next_state, done, probs[action]))  # Store prob of the action\n",
    "\n",
    "        # Train the agent when memory is sufficient\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            agent.train(memory, batch_size=BATCH_SIZE)\n",
    "            memory = []  # Clear memory after training\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{EPISODES}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6f815e2-aa20-4706-8e76-d04d11cab51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO Agent vs Random Agent: 0.6\n",
      "PPO Agent vs Negamax Agent: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate(env_name, agent, opponent, num_episodes=10):\n",
    "    env = make(env_name)\n",
    "    rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.array(state[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = agent.act(state)\n",
    "            valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "            if action not in valid_actions:\n",
    "                action = random.choice(valid_actions)\n",
    "\n",
    "            opponent_action = random.choice(valid_actions)\n",
    "            next_step = env.step([action, opponent_action])\n",
    "            done = env.done\n",
    "            reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "            state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "# Evaluate the agent\n",
    "print(\"PPO Agent vs Random Agent:\", evaluate(\"connectx\", agent, \"random\"))\n",
    "print(\"PPO Agent vs Negamax Agent:\", evaluate(\"connectx\", agent, \"negamax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f4f6f-03ae-4b8c-b126-c64a548a4273",
   "metadata": {},
   "source": [
    "## Analysis of Results:\n",
    "- The results indicate that your PPO Agent performs decently against both the Random Agent and the Negamax Agent, achieving a mean reward of 0.6 for both matchups over the evaluation episodes.\n",
    "\n",
    "1. **PPO Agent vs Random Agent (0.6):** This suggests that the PPO Agent is reliably outperforming a Random Agent, which is a basic benchmark. While this is expected, it doesn't necessarily indicate that the agent has mastered the ConnectX game.\n",
    "\n",
    "2. **PPO Agent vs Negamax Agent (0.6):** Achieving a positive reward against the Negamax Agent (a heuristic-based opponent that makes optimal moves) is promising. However, the score of 0.6 implies there is still room for improvement to dominate a strategic agent like Negamax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821fc0e-9b1f-4b25-b5f7-21bc1dc660fa",
   "metadata": {},
   "source": [
    "###  Hyperparameter Tuning of PPO \n",
    "\n",
    "1. Reward Shaping: Added bonuses for winning moves and penalties for invalid or suboptimal moves.\n",
    "\n",
    "2. Increased Network Capacity: Expanded the hidden layers of the PPO network.\n",
    "\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "- Reduced learning rate (lr=1e-4).\n",
    "- Adjusted gamma to 0.98 for a longer reward horizon.\n",
    "- Clipping parameter set to eps_clip=0.15.\n",
    "- Extended Training: Increased episodes to 200 for better learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7fd8afc-ec68-482f-be37-28a9394971a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DEEPSHIKHA\\AppData\\Local\\Temp\\ipykernel_5328\\2060709579.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2/1000, Total Reward: -1\n",
      "Episode 3/1000, Total Reward: -1\n",
      "Episode 4/1000, Total Reward: 1\n",
      "Episode 5/1000, Total Reward: 1\n",
      "Episode 6/1000, Total Reward: 1\n",
      "Episode 7/1000, Total Reward: 1\n",
      "Episode 8/1000, Total Reward: -1\n",
      "Episode 9/1000, Total Reward: 1\n",
      "Episode 10/1000, Total Reward: 1\n",
      "Episode 11/1000, Total Reward: -1\n",
      "Episode 12/1000, Total Reward: -1\n",
      "Episode 13/1000, Total Reward: -1\n",
      "Episode 14/1000, Total Reward: 1\n",
      "Episode 15/1000, Total Reward: -1\n",
      "Episode 16/1000, Total Reward: -1\n",
      "Episode 17/1000, Total Reward: -1\n",
      "Episode 18/1000, Total Reward: 1\n",
      "Episode 19/1000, Total Reward: 1\n",
      "Episode 20/1000, Total Reward: -1\n",
      "Episode 21/1000, Total Reward: 1\n",
      "Episode 22/1000, Total Reward: -1\n",
      "Episode 23/1000, Total Reward: 1\n",
      "Episode 24/1000, Total Reward: 1\n",
      "Episode 25/1000, Total Reward: -1\n",
      "Episode 26/1000, Total Reward: -1\n",
      "Episode 27/1000, Total Reward: 1\n",
      "Episode 28/1000, Total Reward: -1\n",
      "Episode 29/1000, Total Reward: -1\n",
      "Episode 30/1000, Total Reward: 1\n",
      "Episode 31/1000, Total Reward: 1\n",
      "Episode 32/1000, Total Reward: -1\n",
      "Episode 33/1000, Total Reward: 1\n",
      "Episode 34/1000, Total Reward: 1\n",
      "Episode 35/1000, Total Reward: 1\n",
      "Episode 36/1000, Total Reward: 1\n",
      "Episode 37/1000, Total Reward: 1\n",
      "Episode 38/1000, Total Reward: 1\n",
      "Episode 39/1000, Total Reward: 1\n",
      "Episode 40/1000, Total Reward: -1\n",
      "Episode 41/1000, Total Reward: -1\n",
      "Episode 42/1000, Total Reward: -1\n",
      "Episode 43/1000, Total Reward: 1\n",
      "Episode 44/1000, Total Reward: 1\n",
      "Episode 45/1000, Total Reward: -1\n",
      "Episode 46/1000, Total Reward: 1\n",
      "Episode 47/1000, Total Reward: 1\n",
      "Episode 48/1000, Total Reward: 1\n",
      "Episode 49/1000, Total Reward: 1\n",
      "Episode 50/1000, Total Reward: 1\n",
      "Episode 51/1000, Total Reward: 1\n",
      "Episode 52/1000, Total Reward: -1\n",
      "Episode 53/1000, Total Reward: -1\n",
      "Episode 54/1000, Total Reward: 1\n",
      "Episode 55/1000, Total Reward: 1\n",
      "Episode 56/1000, Total Reward: 1\n",
      "Episode 57/1000, Total Reward: -1\n",
      "Episode 58/1000, Total Reward: -1\n",
      "Episode 59/1000, Total Reward: 1\n",
      "Episode 60/1000, Total Reward: 1\n",
      "Episode 61/1000, Total Reward: -1\n",
      "Episode 62/1000, Total Reward: -1\n",
      "Episode 63/1000, Total Reward: 1\n",
      "Episode 64/1000, Total Reward: 1\n",
      "Episode 65/1000, Total Reward: 1\n",
      "Episode 66/1000, Total Reward: -1\n",
      "Episode 67/1000, Total Reward: -1\n",
      "Episode 68/1000, Total Reward: -1\n",
      "Episode 69/1000, Total Reward: 1\n",
      "Episode 70/1000, Total Reward: -1\n",
      "Episode 71/1000, Total Reward: -1\n",
      "Episode 72/1000, Total Reward: -1\n",
      "Episode 73/1000, Total Reward: -1\n",
      "Episode 74/1000, Total Reward: 1\n",
      "Episode 75/1000, Total Reward: 1\n",
      "Episode 76/1000, Total Reward: -1\n",
      "Episode 77/1000, Total Reward: -1\n",
      "Episode 78/1000, Total Reward: -1\n",
      "Episode 79/1000, Total Reward: -1\n",
      "Episode 80/1000, Total Reward: 1\n",
      "Episode 81/1000, Total Reward: -1\n",
      "Episode 82/1000, Total Reward: -1\n",
      "Episode 83/1000, Total Reward: 1\n",
      "Episode 84/1000, Total Reward: -1\n",
      "Episode 85/1000, Total Reward: -1\n",
      "Episode 86/1000, Total Reward: -1\n",
      "Episode 87/1000, Total Reward: 1\n",
      "Episode 88/1000, Total Reward: 1\n",
      "Episode 89/1000, Total Reward: 1\n",
      "Episode 90/1000, Total Reward: -1\n",
      "Episode 91/1000, Total Reward: 1\n",
      "Episode 92/1000, Total Reward: 1\n",
      "Episode 93/1000, Total Reward: 1\n",
      "Episode 94/1000, Total Reward: -1\n",
      "Episode 95/1000, Total Reward: -1\n",
      "Episode 96/1000, Total Reward: -1\n",
      "Episode 97/1000, Total Reward: -1\n",
      "Episode 98/1000, Total Reward: 1\n",
      "Episode 99/1000, Total Reward: -1\n",
      "Episode 100/1000, Total Reward: 1\n",
      "Episode 101/1000, Total Reward: 1\n",
      "Episode 102/1000, Total Reward: 1\n",
      "Episode 103/1000, Total Reward: 1\n",
      "Episode 104/1000, Total Reward: 1\n",
      "Episode 105/1000, Total Reward: 1\n",
      "Episode 106/1000, Total Reward: -1\n",
      "Episode 107/1000, Total Reward: 1\n",
      "Episode 108/1000, Total Reward: -1\n",
      "Episode 109/1000, Total Reward: -1\n",
      "Episode 110/1000, Total Reward: 1\n",
      "Episode 111/1000, Total Reward: -1\n",
      "Episode 112/1000, Total Reward: -1\n",
      "Episode 113/1000, Total Reward: 1\n",
      "Episode 114/1000, Total Reward: -1\n",
      "Episode 115/1000, Total Reward: -1\n",
      "Episode 116/1000, Total Reward: -1\n",
      "Episode 117/1000, Total Reward: 1\n",
      "Episode 118/1000, Total Reward: 1\n",
      "Episode 119/1000, Total Reward: 1\n",
      "Episode 120/1000, Total Reward: -1\n",
      "Episode 121/1000, Total Reward: -1\n",
      "Episode 122/1000, Total Reward: -1\n",
      "Episode 123/1000, Total Reward: 1\n",
      "Episode 124/1000, Total Reward: 1\n",
      "Episode 125/1000, Total Reward: 1\n",
      "Episode 126/1000, Total Reward: -1\n",
      "Episode 127/1000, Total Reward: 1\n",
      "Episode 128/1000, Total Reward: 1\n",
      "Episode 129/1000, Total Reward: 1\n",
      "Episode 130/1000, Total Reward: 1\n",
      "Episode 131/1000, Total Reward: -1\n",
      "Episode 132/1000, Total Reward: 1\n",
      "Episode 133/1000, Total Reward: -1\n",
      "Episode 134/1000, Total Reward: -1\n",
      "Episode 135/1000, Total Reward: 1\n",
      "Episode 136/1000, Total Reward: 1\n",
      "Episode 137/1000, Total Reward: 1\n",
      "Episode 138/1000, Total Reward: -1\n",
      "Episode 139/1000, Total Reward: 1\n",
      "Episode 140/1000, Total Reward: -1\n",
      "Episode 141/1000, Total Reward: -1\n",
      "Episode 142/1000, Total Reward: -1\n",
      "Episode 143/1000, Total Reward: 1\n",
      "Episode 144/1000, Total Reward: 1\n",
      "Episode 145/1000, Total Reward: -1\n",
      "Episode 146/1000, Total Reward: -1\n",
      "Episode 147/1000, Total Reward: -1\n",
      "Episode 148/1000, Total Reward: -1\n",
      "Episode 149/1000, Total Reward: 1\n",
      "Episode 150/1000, Total Reward: -1\n",
      "Episode 151/1000, Total Reward: -1\n",
      "Episode 152/1000, Total Reward: -1\n",
      "Episode 153/1000, Total Reward: -1\n",
      "Episode 154/1000, Total Reward: 1\n",
      "Episode 155/1000, Total Reward: 1\n",
      "Episode 156/1000, Total Reward: 1\n",
      "Episode 157/1000, Total Reward: -1\n",
      "Episode 158/1000, Total Reward: -1\n",
      "Episode 159/1000, Total Reward: -1\n",
      "Episode 160/1000, Total Reward: 1\n",
      "Episode 161/1000, Total Reward: 1\n",
      "Episode 162/1000, Total Reward: 1\n",
      "Episode 163/1000, Total Reward: -1\n",
      "Episode 164/1000, Total Reward: 1\n",
      "Episode 165/1000, Total Reward: 1\n",
      "Episode 166/1000, Total Reward: -1\n",
      "Episode 167/1000, Total Reward: -1\n",
      "Episode 168/1000, Total Reward: -1\n",
      "Episode 169/1000, Total Reward: 1\n",
      "Episode 170/1000, Total Reward: -1\n",
      "Episode 171/1000, Total Reward: 1\n",
      "Episode 172/1000, Total Reward: -1\n",
      "Episode 173/1000, Total Reward: 1\n",
      "Episode 174/1000, Total Reward: -1\n",
      "Episode 175/1000, Total Reward: 1\n",
      "Episode 176/1000, Total Reward: 1\n",
      "Episode 177/1000, Total Reward: -1\n",
      "Episode 178/1000, Total Reward: -1\n",
      "Episode 179/1000, Total Reward: -1\n",
      "Episode 180/1000, Total Reward: -1\n",
      "Episode 181/1000, Total Reward: -1\n",
      "Episode 182/1000, Total Reward: 1\n",
      "Episode 183/1000, Total Reward: -1\n",
      "Episode 184/1000, Total Reward: -1\n",
      "Episode 185/1000, Total Reward: 1\n",
      "Episode 186/1000, Total Reward: 1\n",
      "Episode 187/1000, Total Reward: 1\n",
      "Episode 188/1000, Total Reward: -1\n",
      "Episode 189/1000, Total Reward: -1\n",
      "Episode 190/1000, Total Reward: 1\n",
      "Episode 191/1000, Total Reward: -1\n",
      "Episode 192/1000, Total Reward: -1\n",
      "Episode 193/1000, Total Reward: 1\n",
      "Episode 194/1000, Total Reward: 1\n",
      "Episode 195/1000, Total Reward: -1\n",
      "Episode 196/1000, Total Reward: 1\n",
      "Episode 197/1000, Total Reward: -1\n",
      "Episode 198/1000, Total Reward: -1\n",
      "Episode 199/1000, Total Reward: -1\n",
      "Episode 200/1000, Total Reward: -1\n",
      "Episode 201/1000, Total Reward: -1\n",
      "Episode 202/1000, Total Reward: 1\n",
      "Episode 203/1000, Total Reward: 1\n",
      "Episode 204/1000, Total Reward: -1\n",
      "Episode 205/1000, Total Reward: -1\n",
      "Episode 206/1000, Total Reward: -1\n",
      "Episode 207/1000, Total Reward: 1\n",
      "Episode 208/1000, Total Reward: -1\n",
      "Episode 209/1000, Total Reward: 1\n",
      "Episode 210/1000, Total Reward: -1\n",
      "Episode 211/1000, Total Reward: -1\n",
      "Episode 212/1000, Total Reward: 1\n",
      "Episode 213/1000, Total Reward: -1\n",
      "Episode 214/1000, Total Reward: -1\n",
      "Episode 215/1000, Total Reward: 1\n",
      "Episode 216/1000, Total Reward: 1\n",
      "Episode 217/1000, Total Reward: 1\n",
      "Episode 218/1000, Total Reward: -1\n",
      "Episode 219/1000, Total Reward: 1\n",
      "Episode 220/1000, Total Reward: 1\n",
      "Episode 221/1000, Total Reward: -1\n",
      "Episode 222/1000, Total Reward: -1\n",
      "Episode 223/1000, Total Reward: -1\n",
      "Episode 224/1000, Total Reward: 1\n",
      "Episode 225/1000, Total Reward: -1\n",
      "Episode 226/1000, Total Reward: 1\n",
      "Episode 227/1000, Total Reward: 1\n",
      "Episode 228/1000, Total Reward: -1\n",
      "Episode 229/1000, Total Reward: 1\n",
      "Episode 230/1000, Total Reward: -1\n",
      "Episode 231/1000, Total Reward: 1\n",
      "Episode 232/1000, Total Reward: -1\n",
      "Episode 233/1000, Total Reward: -1\n",
      "Episode 234/1000, Total Reward: 1\n",
      "Episode 235/1000, Total Reward: 1\n",
      "Episode 236/1000, Total Reward: 1\n",
      "Episode 237/1000, Total Reward: 1\n",
      "Episode 238/1000, Total Reward: -1\n",
      "Episode 239/1000, Total Reward: -1\n",
      "Episode 240/1000, Total Reward: -1\n",
      "Episode 241/1000, Total Reward: -1\n",
      "Episode 242/1000, Total Reward: -1\n",
      "Episode 243/1000, Total Reward: -1\n",
      "Episode 244/1000, Total Reward: -1\n",
      "Episode 245/1000, Total Reward: -1\n",
      "Episode 246/1000, Total Reward: -1\n",
      "Episode 247/1000, Total Reward: 1\n",
      "Episode 248/1000, Total Reward: 1\n",
      "Episode 249/1000, Total Reward: -1\n",
      "Episode 250/1000, Total Reward: 1\n",
      "Episode 251/1000, Total Reward: 1\n",
      "Episode 252/1000, Total Reward: -1\n",
      "Episode 253/1000, Total Reward: 1\n",
      "Episode 254/1000, Total Reward: -1\n",
      "Episode 255/1000, Total Reward: -1\n",
      "Episode 256/1000, Total Reward: -1\n",
      "Episode 257/1000, Total Reward: 1\n",
      "Episode 258/1000, Total Reward: -1\n",
      "Episode 259/1000, Total Reward: -1\n",
      "Episode 260/1000, Total Reward: 1\n",
      "Episode 261/1000, Total Reward: 1\n",
      "Episode 262/1000, Total Reward: 1\n",
      "Episode 263/1000, Total Reward: 1\n",
      "Episode 264/1000, Total Reward: 1\n",
      "Episode 265/1000, Total Reward: 1\n",
      "Episode 266/1000, Total Reward: 1\n",
      "Episode 267/1000, Total Reward: 1\n",
      "Episode 268/1000, Total Reward: 1\n",
      "Episode 269/1000, Total Reward: 1\n",
      "Episode 270/1000, Total Reward: 1\n",
      "Episode 271/1000, Total Reward: 1\n",
      "Episode 272/1000, Total Reward: -1\n",
      "Episode 273/1000, Total Reward: -1\n",
      "Episode 274/1000, Total Reward: 1\n",
      "Episode 275/1000, Total Reward: 1\n",
      "Episode 276/1000, Total Reward: 1\n",
      "Episode 277/1000, Total Reward: 1\n",
      "Episode 278/1000, Total Reward: 1\n",
      "Episode 279/1000, Total Reward: 1\n",
      "Episode 280/1000, Total Reward: 1\n",
      "Episode 281/1000, Total Reward: 1\n",
      "Episode 282/1000, Total Reward: -1\n",
      "Episode 283/1000, Total Reward: 1\n",
      "Episode 284/1000, Total Reward: 1\n",
      "Episode 285/1000, Total Reward: -1\n",
      "Episode 286/1000, Total Reward: 1\n",
      "Episode 287/1000, Total Reward: -1\n",
      "Episode 288/1000, Total Reward: 1\n",
      "Episode 289/1000, Total Reward: 1\n",
      "Episode 290/1000, Total Reward: -1\n",
      "Episode 291/1000, Total Reward: 1\n",
      "Episode 292/1000, Total Reward: 1\n",
      "Episode 293/1000, Total Reward: 1\n",
      "Episode 294/1000, Total Reward: -1\n",
      "Episode 295/1000, Total Reward: 1\n",
      "Episode 296/1000, Total Reward: 1\n",
      "Episode 297/1000, Total Reward: 1\n",
      "Episode 298/1000, Total Reward: 1\n",
      "Episode 299/1000, Total Reward: -1\n",
      "Episode 300/1000, Total Reward: -1\n",
      "Episode 301/1000, Total Reward: -1\n",
      "Episode 302/1000, Total Reward: 1\n",
      "Episode 303/1000, Total Reward: -1\n",
      "Episode 304/1000, Total Reward: -1\n",
      "Episode 305/1000, Total Reward: -1\n",
      "Episode 306/1000, Total Reward: -1\n",
      "Episode 307/1000, Total Reward: 1\n",
      "Episode 308/1000, Total Reward: -1\n",
      "Episode 309/1000, Total Reward: 1\n",
      "Episode 310/1000, Total Reward: 1\n",
      "Episode 311/1000, Total Reward: -1\n",
      "Episode 312/1000, Total Reward: 1\n",
      "Episode 313/1000, Total Reward: 1\n",
      "Episode 314/1000, Total Reward: 1\n",
      "Episode 315/1000, Total Reward: -1\n",
      "Episode 316/1000, Total Reward: 1\n",
      "Episode 317/1000, Total Reward: -1\n",
      "Episode 318/1000, Total Reward: 1\n",
      "Episode 319/1000, Total Reward: -1\n",
      "Episode 320/1000, Total Reward: -1\n",
      "Episode 321/1000, Total Reward: -1\n",
      "Episode 322/1000, Total Reward: -1\n",
      "Episode 323/1000, Total Reward: -1\n",
      "Episode 324/1000, Total Reward: -1\n",
      "Episode 325/1000, Total Reward: -1\n",
      "Episode 326/1000, Total Reward: 1\n",
      "Episode 327/1000, Total Reward: -1\n",
      "Episode 328/1000, Total Reward: 1\n",
      "Episode 329/1000, Total Reward: 1\n",
      "Episode 330/1000, Total Reward: -1\n",
      "Episode 331/1000, Total Reward: 0\n",
      "Episode 332/1000, Total Reward: 1\n",
      "Episode 333/1000, Total Reward: 1\n",
      "Episode 334/1000, Total Reward: 1\n",
      "Episode 335/1000, Total Reward: -1\n",
      "Episode 336/1000, Total Reward: 1\n",
      "Episode 337/1000, Total Reward: 1\n",
      "Episode 338/1000, Total Reward: 1\n",
      "Episode 339/1000, Total Reward: -1\n",
      "Episode 340/1000, Total Reward: -1\n",
      "Episode 341/1000, Total Reward: 1\n",
      "Episode 342/1000, Total Reward: 1\n",
      "Episode 343/1000, Total Reward: 1\n",
      "Episode 344/1000, Total Reward: 1\n",
      "Episode 345/1000, Total Reward: 1\n",
      "Episode 346/1000, Total Reward: 1\n",
      "Episode 347/1000, Total Reward: -1\n",
      "Episode 348/1000, Total Reward: 1\n",
      "Episode 349/1000, Total Reward: 1\n",
      "Episode 350/1000, Total Reward: 1\n",
      "Episode 351/1000, Total Reward: 1\n",
      "Episode 352/1000, Total Reward: 1\n",
      "Episode 353/1000, Total Reward: -1\n",
      "Episode 354/1000, Total Reward: 1\n",
      "Episode 355/1000, Total Reward: -1\n",
      "Episode 356/1000, Total Reward: -1\n",
      "Episode 357/1000, Total Reward: 1\n",
      "Episode 358/1000, Total Reward: -1\n",
      "Episode 359/1000, Total Reward: -1\n",
      "Episode 360/1000, Total Reward: -1\n",
      "Episode 361/1000, Total Reward: 1\n",
      "Episode 362/1000, Total Reward: -1\n",
      "Episode 363/1000, Total Reward: -1\n",
      "Episode 364/1000, Total Reward: 1\n",
      "Episode 365/1000, Total Reward: -1\n",
      "Episode 366/1000, Total Reward: 1\n",
      "Episode 367/1000, Total Reward: -1\n",
      "Episode 368/1000, Total Reward: 1\n",
      "Episode 369/1000, Total Reward: 1\n",
      "Episode 370/1000, Total Reward: -1\n",
      "Episode 371/1000, Total Reward: -1\n",
      "Episode 372/1000, Total Reward: -1\n",
      "Episode 373/1000, Total Reward: 1\n",
      "Episode 374/1000, Total Reward: -1\n",
      "Episode 375/1000, Total Reward: -1\n",
      "Episode 376/1000, Total Reward: 1\n",
      "Episode 377/1000, Total Reward: 1\n",
      "Episode 378/1000, Total Reward: 1\n",
      "Episode 379/1000, Total Reward: -1\n",
      "Episode 380/1000, Total Reward: 1\n",
      "Episode 381/1000, Total Reward: 1\n",
      "Episode 382/1000, Total Reward: 1\n",
      "Episode 383/1000, Total Reward: 1\n",
      "Episode 384/1000, Total Reward: 1\n",
      "Episode 385/1000, Total Reward: 1\n",
      "Episode 386/1000, Total Reward: 1\n",
      "Episode 387/1000, Total Reward: 1\n",
      "Episode 388/1000, Total Reward: -1\n",
      "Episode 389/1000, Total Reward: 1\n",
      "Episode 390/1000, Total Reward: -1\n",
      "Episode 391/1000, Total Reward: -1\n",
      "Episode 392/1000, Total Reward: 1\n",
      "Episode 393/1000, Total Reward: 1\n",
      "Episode 394/1000, Total Reward: 1\n",
      "Episode 395/1000, Total Reward: 1\n",
      "Episode 396/1000, Total Reward: 1\n",
      "Episode 397/1000, Total Reward: 1\n",
      "Episode 398/1000, Total Reward: 1\n",
      "Episode 399/1000, Total Reward: 1\n",
      "Episode 400/1000, Total Reward: -1\n",
      "Episode 401/1000, Total Reward: -1\n",
      "Episode 402/1000, Total Reward: 1\n",
      "Episode 403/1000, Total Reward: -1\n",
      "Episode 404/1000, Total Reward: 1\n",
      "Episode 405/1000, Total Reward: 1\n",
      "Episode 406/1000, Total Reward: 1\n",
      "Episode 407/1000, Total Reward: 1\n",
      "Episode 408/1000, Total Reward: 1\n",
      "Episode 409/1000, Total Reward: 1\n",
      "Episode 410/1000, Total Reward: -1\n",
      "Episode 411/1000, Total Reward: 1\n",
      "Episode 412/1000, Total Reward: 1\n",
      "Episode 413/1000, Total Reward: 1\n",
      "Episode 414/1000, Total Reward: 1\n",
      "Episode 415/1000, Total Reward: 1\n",
      "Episode 416/1000, Total Reward: -1\n",
      "Episode 417/1000, Total Reward: 1\n",
      "Episode 418/1000, Total Reward: 1\n",
      "Episode 419/1000, Total Reward: 1\n",
      "Episode 420/1000, Total Reward: 1\n",
      "Episode 421/1000, Total Reward: 1\n",
      "Episode 422/1000, Total Reward: 1\n",
      "Episode 423/1000, Total Reward: -1\n",
      "Episode 424/1000, Total Reward: 1\n",
      "Episode 425/1000, Total Reward: -1\n",
      "Episode 426/1000, Total Reward: -1\n",
      "Episode 427/1000, Total Reward: 1\n",
      "Episode 428/1000, Total Reward: -1\n",
      "Episode 429/1000, Total Reward: 1\n",
      "Episode 430/1000, Total Reward: -1\n",
      "Episode 431/1000, Total Reward: 1\n",
      "Episode 432/1000, Total Reward: 1\n",
      "Episode 433/1000, Total Reward: -1\n",
      "Episode 434/1000, Total Reward: 1\n",
      "Episode 435/1000, Total Reward: 1\n",
      "Episode 436/1000, Total Reward: 1\n",
      "Episode 437/1000, Total Reward: 1\n",
      "Episode 438/1000, Total Reward: 1\n",
      "Episode 439/1000, Total Reward: 1\n",
      "Episode 440/1000, Total Reward: 1\n",
      "Episode 441/1000, Total Reward: -1\n",
      "Episode 442/1000, Total Reward: -1\n",
      "Episode 443/1000, Total Reward: -1\n",
      "Episode 444/1000, Total Reward: 1\n",
      "Episode 445/1000, Total Reward: 1\n",
      "Episode 446/1000, Total Reward: 1\n",
      "Episode 447/1000, Total Reward: 1\n",
      "Episode 448/1000, Total Reward: 1\n",
      "Episode 449/1000, Total Reward: -1\n",
      "Episode 450/1000, Total Reward: 1\n",
      "Episode 451/1000, Total Reward: -1\n",
      "Episode 452/1000, Total Reward: -1\n",
      "Episode 453/1000, Total Reward: -1\n",
      "Episode 454/1000, Total Reward: -1\n",
      "Episode 455/1000, Total Reward: 1\n",
      "Episode 456/1000, Total Reward: -1\n",
      "Episode 457/1000, Total Reward: -1\n",
      "Episode 458/1000, Total Reward: 1\n",
      "Episode 459/1000, Total Reward: 1\n",
      "Episode 460/1000, Total Reward: 1\n",
      "Episode 461/1000, Total Reward: 1\n",
      "Episode 462/1000, Total Reward: -1\n",
      "Episode 463/1000, Total Reward: 1\n",
      "Episode 464/1000, Total Reward: 1\n",
      "Episode 465/1000, Total Reward: 1\n",
      "Episode 466/1000, Total Reward: 1\n",
      "Episode 467/1000, Total Reward: 1\n",
      "Episode 468/1000, Total Reward: 1\n",
      "Episode 469/1000, Total Reward: 1\n",
      "Episode 470/1000, Total Reward: 1\n",
      "Episode 471/1000, Total Reward: 1\n",
      "Episode 472/1000, Total Reward: 1\n",
      "Episode 473/1000, Total Reward: -1\n",
      "Episode 474/1000, Total Reward: 1\n",
      "Episode 475/1000, Total Reward: 1\n",
      "Episode 476/1000, Total Reward: 1\n",
      "Episode 477/1000, Total Reward: 1\n",
      "Episode 478/1000, Total Reward: -1\n",
      "Episode 479/1000, Total Reward: 1\n",
      "Episode 480/1000, Total Reward: 1\n",
      "Episode 481/1000, Total Reward: -1\n",
      "Episode 482/1000, Total Reward: 1\n",
      "Episode 483/1000, Total Reward: -1\n",
      "Episode 484/1000, Total Reward: 1\n",
      "Episode 485/1000, Total Reward: 1\n",
      "Episode 486/1000, Total Reward: 1\n",
      "Episode 487/1000, Total Reward: 1\n",
      "Episode 488/1000, Total Reward: 1\n",
      "Episode 489/1000, Total Reward: -1\n",
      "Episode 490/1000, Total Reward: 1\n",
      "Episode 491/1000, Total Reward: -1\n",
      "Episode 492/1000, Total Reward: 1\n",
      "Episode 493/1000, Total Reward: 1\n",
      "Episode 494/1000, Total Reward: -1\n",
      "Episode 495/1000, Total Reward: 1\n",
      "Episode 496/1000, Total Reward: -1\n",
      "Episode 497/1000, Total Reward: 1\n",
      "Episode 498/1000, Total Reward: -1\n",
      "Episode 499/1000, Total Reward: -1\n",
      "Episode 500/1000, Total Reward: -1\n",
      "Episode 501/1000, Total Reward: -1\n",
      "Episode 502/1000, Total Reward: 1\n",
      "Episode 503/1000, Total Reward: 1\n",
      "Episode 504/1000, Total Reward: -1\n",
      "Episode 505/1000, Total Reward: -1\n",
      "Episode 506/1000, Total Reward: 1\n",
      "Episode 507/1000, Total Reward: -1\n",
      "Episode 508/1000, Total Reward: -1\n",
      "Episode 509/1000, Total Reward: -1\n",
      "Episode 510/1000, Total Reward: 1\n",
      "Episode 511/1000, Total Reward: 1\n",
      "Episode 512/1000, Total Reward: -1\n",
      "Episode 513/1000, Total Reward: -1\n",
      "Episode 514/1000, Total Reward: 1\n",
      "Episode 515/1000, Total Reward: -1\n",
      "Episode 516/1000, Total Reward: 1\n",
      "Episode 517/1000, Total Reward: -1\n",
      "Episode 518/1000, Total Reward: 1\n",
      "Episode 519/1000, Total Reward: 1\n",
      "Episode 520/1000, Total Reward: 1\n",
      "Episode 521/1000, Total Reward: -1\n",
      "Episode 522/1000, Total Reward: -1\n",
      "Episode 523/1000, Total Reward: -1\n",
      "Episode 524/1000, Total Reward: 1\n",
      "Episode 525/1000, Total Reward: -1\n",
      "Episode 526/1000, Total Reward: -1\n",
      "Episode 527/1000, Total Reward: 1\n",
      "Episode 528/1000, Total Reward: -1\n",
      "Episode 529/1000, Total Reward: -1\n",
      "Episode 530/1000, Total Reward: 1\n",
      "Episode 531/1000, Total Reward: 1\n",
      "Episode 532/1000, Total Reward: 1\n",
      "Episode 533/1000, Total Reward: -1\n",
      "Episode 534/1000, Total Reward: 1\n",
      "Episode 535/1000, Total Reward: -1\n",
      "Episode 536/1000, Total Reward: -1\n",
      "Episode 537/1000, Total Reward: -1\n",
      "Episode 538/1000, Total Reward: -1\n",
      "Episode 539/1000, Total Reward: -1\n",
      "Episode 540/1000, Total Reward: -1\n",
      "Episode 541/1000, Total Reward: -1\n",
      "Episode 542/1000, Total Reward: 1\n",
      "Episode 543/1000, Total Reward: 1\n",
      "Episode 544/1000, Total Reward: -1\n",
      "Episode 545/1000, Total Reward: 1\n",
      "Episode 546/1000, Total Reward: 1\n",
      "Episode 547/1000, Total Reward: -1\n",
      "Episode 548/1000, Total Reward: -1\n",
      "Episode 549/1000, Total Reward: -1\n",
      "Episode 550/1000, Total Reward: -1\n",
      "Episode 551/1000, Total Reward: -1\n",
      "Episode 552/1000, Total Reward: 1\n",
      "Episode 553/1000, Total Reward: 1\n",
      "Episode 554/1000, Total Reward: 1\n",
      "Episode 555/1000, Total Reward: 1\n",
      "Episode 556/1000, Total Reward: 1\n",
      "Episode 557/1000, Total Reward: -1\n",
      "Episode 558/1000, Total Reward: -1\n",
      "Episode 559/1000, Total Reward: -1\n",
      "Episode 560/1000, Total Reward: -1\n",
      "Episode 561/1000, Total Reward: -1\n",
      "Episode 562/1000, Total Reward: -1\n",
      "Episode 563/1000, Total Reward: 1\n",
      "Episode 564/1000, Total Reward: -1\n",
      "Episode 565/1000, Total Reward: 0\n",
      "Episode 566/1000, Total Reward: 1\n",
      "Episode 567/1000, Total Reward: 1\n",
      "Episode 568/1000, Total Reward: -1\n",
      "Episode 569/1000, Total Reward: 1\n",
      "Episode 570/1000, Total Reward: 1\n",
      "Episode 571/1000, Total Reward: -1\n",
      "Episode 572/1000, Total Reward: 1\n",
      "Episode 573/1000, Total Reward: 1\n",
      "Episode 574/1000, Total Reward: 1\n",
      "Episode 575/1000, Total Reward: -1\n",
      "Episode 576/1000, Total Reward: -1\n",
      "Episode 577/1000, Total Reward: 1\n",
      "Episode 578/1000, Total Reward: -1\n",
      "Episode 579/1000, Total Reward: -1\n",
      "Episode 580/1000, Total Reward: -1\n",
      "Episode 581/1000, Total Reward: 1\n",
      "Episode 582/1000, Total Reward: -1\n",
      "Episode 583/1000, Total Reward: -1\n",
      "Episode 584/1000, Total Reward: -1\n",
      "Episode 585/1000, Total Reward: -1\n",
      "Episode 586/1000, Total Reward: 1\n",
      "Episode 587/1000, Total Reward: -1\n",
      "Episode 588/1000, Total Reward: 1\n",
      "Episode 589/1000, Total Reward: -1\n",
      "Episode 590/1000, Total Reward: -1\n",
      "Episode 591/1000, Total Reward: 1\n",
      "Episode 592/1000, Total Reward: -1\n",
      "Episode 593/1000, Total Reward: 1\n",
      "Episode 594/1000, Total Reward: -1\n",
      "Episode 595/1000, Total Reward: -1\n",
      "Episode 596/1000, Total Reward: 1\n",
      "Episode 597/1000, Total Reward: -1\n",
      "Episode 598/1000, Total Reward: 1\n",
      "Episode 599/1000, Total Reward: -1\n",
      "Episode 600/1000, Total Reward: -1\n",
      "Episode 601/1000, Total Reward: -1\n",
      "Episode 602/1000, Total Reward: -1\n",
      "Episode 603/1000, Total Reward: -1\n",
      "Episode 604/1000, Total Reward: -1\n",
      "Episode 605/1000, Total Reward: -1\n",
      "Episode 606/1000, Total Reward: -1\n",
      "Episode 607/1000, Total Reward: 1\n",
      "Episode 608/1000, Total Reward: -1\n",
      "Episode 609/1000, Total Reward: 1\n",
      "Episode 610/1000, Total Reward: 1\n",
      "Episode 611/1000, Total Reward: 1\n",
      "Episode 612/1000, Total Reward: 1\n",
      "Episode 613/1000, Total Reward: 1\n",
      "Episode 614/1000, Total Reward: 1\n",
      "Episode 615/1000, Total Reward: -1\n",
      "Episode 616/1000, Total Reward: 1\n",
      "Episode 617/1000, Total Reward: -1\n",
      "Episode 618/1000, Total Reward: 1\n",
      "Episode 619/1000, Total Reward: 1\n",
      "Episode 620/1000, Total Reward: -1\n",
      "Episode 621/1000, Total Reward: -1\n",
      "Episode 622/1000, Total Reward: -1\n",
      "Episode 623/1000, Total Reward: -1\n",
      "Episode 624/1000, Total Reward: 1\n",
      "Episode 625/1000, Total Reward: -1\n",
      "Episode 626/1000, Total Reward: -1\n",
      "Episode 627/1000, Total Reward: -1\n",
      "Episode 628/1000, Total Reward: -1\n",
      "Episode 629/1000, Total Reward: 1\n",
      "Episode 630/1000, Total Reward: -1\n",
      "Episode 631/1000, Total Reward: -1\n",
      "Episode 632/1000, Total Reward: 1\n",
      "Episode 633/1000, Total Reward: 1\n",
      "Episode 634/1000, Total Reward: 1\n",
      "Episode 635/1000, Total Reward: 1\n",
      "Episode 636/1000, Total Reward: 1\n",
      "Episode 637/1000, Total Reward: -1\n",
      "Episode 638/1000, Total Reward: -1\n",
      "Episode 639/1000, Total Reward: 1\n",
      "Episode 640/1000, Total Reward: 1\n",
      "Episode 641/1000, Total Reward: 1\n",
      "Episode 642/1000, Total Reward: 1\n",
      "Episode 643/1000, Total Reward: 1\n",
      "Episode 644/1000, Total Reward: 1\n",
      "Episode 645/1000, Total Reward: -1\n",
      "Episode 646/1000, Total Reward: -1\n",
      "Episode 647/1000, Total Reward: 1\n",
      "Episode 648/1000, Total Reward: 1\n",
      "Episode 649/1000, Total Reward: 1\n",
      "Episode 650/1000, Total Reward: -1\n",
      "Episode 651/1000, Total Reward: -1\n",
      "Episode 652/1000, Total Reward: 1\n",
      "Episode 653/1000, Total Reward: -1\n",
      "Episode 654/1000, Total Reward: 1\n",
      "Episode 655/1000, Total Reward: 1\n",
      "Episode 656/1000, Total Reward: -1\n",
      "Episode 657/1000, Total Reward: 1\n",
      "Episode 658/1000, Total Reward: 1\n",
      "Episode 659/1000, Total Reward: 1\n",
      "Episode 660/1000, Total Reward: 1\n",
      "Episode 661/1000, Total Reward: -1\n",
      "Episode 662/1000, Total Reward: 1\n",
      "Episode 663/1000, Total Reward: -1\n",
      "Episode 664/1000, Total Reward: 1\n",
      "Episode 665/1000, Total Reward: -1\n",
      "Episode 666/1000, Total Reward: 1\n",
      "Episode 667/1000, Total Reward: 1\n",
      "Episode 668/1000, Total Reward: -1\n",
      "Episode 669/1000, Total Reward: -1\n",
      "Episode 670/1000, Total Reward: 1\n",
      "Episode 671/1000, Total Reward: -1\n",
      "Episode 672/1000, Total Reward: 1\n",
      "Episode 673/1000, Total Reward: -1\n",
      "Episode 674/1000, Total Reward: 1\n",
      "Episode 675/1000, Total Reward: -1\n",
      "Episode 676/1000, Total Reward: 1\n",
      "Episode 677/1000, Total Reward: -1\n",
      "Episode 678/1000, Total Reward: -1\n",
      "Episode 679/1000, Total Reward: -1\n",
      "Episode 680/1000, Total Reward: -1\n",
      "Episode 681/1000, Total Reward: 1\n",
      "Episode 682/1000, Total Reward: -1\n",
      "Episode 683/1000, Total Reward: 1\n",
      "Episode 684/1000, Total Reward: 1\n",
      "Episode 685/1000, Total Reward: -1\n",
      "Episode 686/1000, Total Reward: 1\n",
      "Episode 687/1000, Total Reward: 1\n",
      "Episode 688/1000, Total Reward: 1\n",
      "Episode 689/1000, Total Reward: -1\n",
      "Episode 690/1000, Total Reward: 1\n",
      "Episode 691/1000, Total Reward: -1\n",
      "Episode 692/1000, Total Reward: -1\n",
      "Episode 693/1000, Total Reward: 1\n",
      "Episode 694/1000, Total Reward: 1\n",
      "Episode 695/1000, Total Reward: 1\n",
      "Episode 696/1000, Total Reward: -1\n",
      "Episode 697/1000, Total Reward: -1\n",
      "Episode 698/1000, Total Reward: -1\n",
      "Episode 699/1000, Total Reward: -1\n",
      "Episode 700/1000, Total Reward: 1\n",
      "Episode 701/1000, Total Reward: 1\n",
      "Episode 702/1000, Total Reward: 1\n",
      "Episode 703/1000, Total Reward: 1\n",
      "Episode 704/1000, Total Reward: 1\n",
      "Episode 705/1000, Total Reward: 1\n",
      "Episode 706/1000, Total Reward: 1\n",
      "Episode 707/1000, Total Reward: 1\n",
      "Episode 708/1000, Total Reward: 1\n",
      "Episode 709/1000, Total Reward: -1\n",
      "Episode 710/1000, Total Reward: -1\n",
      "Episode 711/1000, Total Reward: 1\n",
      "Episode 712/1000, Total Reward: 1\n",
      "Episode 713/1000, Total Reward: 1\n",
      "Episode 714/1000, Total Reward: -1\n",
      "Episode 715/1000, Total Reward: 1\n",
      "Episode 716/1000, Total Reward: -1\n",
      "Episode 717/1000, Total Reward: -1\n",
      "Episode 718/1000, Total Reward: -1\n",
      "Episode 719/1000, Total Reward: -1\n",
      "Episode 720/1000, Total Reward: -1\n",
      "Episode 721/1000, Total Reward: -1\n",
      "Episode 722/1000, Total Reward: 1\n",
      "Episode 723/1000, Total Reward: -1\n",
      "Episode 724/1000, Total Reward: -1\n",
      "Episode 725/1000, Total Reward: 1\n",
      "Episode 726/1000, Total Reward: -1\n",
      "Episode 727/1000, Total Reward: 1\n",
      "Episode 728/1000, Total Reward: -1\n",
      "Episode 729/1000, Total Reward: 1\n",
      "Episode 730/1000, Total Reward: -1\n",
      "Episode 731/1000, Total Reward: 1\n",
      "Episode 732/1000, Total Reward: 1\n",
      "Episode 733/1000, Total Reward: -1\n",
      "Episode 734/1000, Total Reward: 1\n",
      "Episode 735/1000, Total Reward: 1\n",
      "Episode 736/1000, Total Reward: -1\n",
      "Episode 737/1000, Total Reward: -1\n",
      "Episode 738/1000, Total Reward: 1\n",
      "Episode 739/1000, Total Reward: -1\n",
      "Episode 740/1000, Total Reward: -1\n",
      "Episode 741/1000, Total Reward: 1\n",
      "Episode 742/1000, Total Reward: 1\n",
      "Episode 743/1000, Total Reward: 1\n",
      "Episode 744/1000, Total Reward: 1\n",
      "Episode 745/1000, Total Reward: -1\n",
      "Episode 746/1000, Total Reward: 1\n",
      "Episode 747/1000, Total Reward: -1\n",
      "Episode 748/1000, Total Reward: 1\n",
      "Episode 749/1000, Total Reward: 1\n",
      "Episode 750/1000, Total Reward: 1\n",
      "Episode 751/1000, Total Reward: -1\n",
      "Episode 752/1000, Total Reward: -1\n",
      "Episode 753/1000, Total Reward: 1\n",
      "Episode 754/1000, Total Reward: 1\n",
      "Episode 755/1000, Total Reward: -1\n",
      "Episode 756/1000, Total Reward: 1\n",
      "Episode 757/1000, Total Reward: 1\n",
      "Episode 758/1000, Total Reward: -1\n",
      "Episode 759/1000, Total Reward: -1\n",
      "Episode 760/1000, Total Reward: 1\n",
      "Episode 761/1000, Total Reward: -1\n",
      "Episode 762/1000, Total Reward: -1\n",
      "Episode 763/1000, Total Reward: 1\n",
      "Episode 764/1000, Total Reward: 1\n",
      "Episode 765/1000, Total Reward: 1\n",
      "Episode 766/1000, Total Reward: 1\n",
      "Episode 767/1000, Total Reward: -1\n",
      "Episode 768/1000, Total Reward: 1\n",
      "Episode 769/1000, Total Reward: -1\n",
      "Episode 770/1000, Total Reward: -1\n",
      "Episode 771/1000, Total Reward: 1\n",
      "Episode 772/1000, Total Reward: 1\n",
      "Episode 773/1000, Total Reward: 1\n",
      "Episode 774/1000, Total Reward: -1\n",
      "Episode 775/1000, Total Reward: -1\n",
      "Episode 776/1000, Total Reward: 1\n",
      "Episode 777/1000, Total Reward: -1\n",
      "Episode 778/1000, Total Reward: -1\n",
      "Episode 779/1000, Total Reward: 1\n",
      "Episode 780/1000, Total Reward: -1\n",
      "Episode 781/1000, Total Reward: -1\n",
      "Episode 782/1000, Total Reward: -1\n",
      "Episode 783/1000, Total Reward: -1\n",
      "Episode 784/1000, Total Reward: 1\n",
      "Episode 785/1000, Total Reward: 1\n",
      "Episode 786/1000, Total Reward: 1\n",
      "Episode 787/1000, Total Reward: 1\n",
      "Episode 788/1000, Total Reward: -1\n",
      "Episode 789/1000, Total Reward: -1\n",
      "Episode 790/1000, Total Reward: 1\n",
      "Episode 791/1000, Total Reward: 1\n",
      "Episode 792/1000, Total Reward: -1\n",
      "Episode 793/1000, Total Reward: 1\n",
      "Episode 794/1000, Total Reward: -1\n",
      "Episode 795/1000, Total Reward: 1\n",
      "Episode 796/1000, Total Reward: -1\n",
      "Episode 797/1000, Total Reward: -1\n",
      "Episode 798/1000, Total Reward: 1\n",
      "Episode 799/1000, Total Reward: -1\n",
      "Episode 800/1000, Total Reward: -1\n",
      "Episode 801/1000, Total Reward: -1\n",
      "Episode 802/1000, Total Reward: 1\n",
      "Episode 803/1000, Total Reward: 1\n",
      "Episode 804/1000, Total Reward: -1\n",
      "Episode 805/1000, Total Reward: 1\n",
      "Episode 806/1000, Total Reward: 1\n",
      "Episode 807/1000, Total Reward: 1\n",
      "Episode 808/1000, Total Reward: 1\n",
      "Episode 809/1000, Total Reward: -1\n",
      "Episode 810/1000, Total Reward: -1\n",
      "Episode 811/1000, Total Reward: 1\n",
      "Episode 812/1000, Total Reward: 1\n",
      "Episode 813/1000, Total Reward: 1\n",
      "Episode 814/1000, Total Reward: -1\n",
      "Episode 815/1000, Total Reward: -1\n",
      "Episode 816/1000, Total Reward: 1\n",
      "Episode 817/1000, Total Reward: 1\n",
      "Episode 818/1000, Total Reward: -1\n",
      "Episode 819/1000, Total Reward: 1\n",
      "Episode 820/1000, Total Reward: 1\n",
      "Episode 821/1000, Total Reward: -1\n",
      "Episode 822/1000, Total Reward: 1\n",
      "Episode 823/1000, Total Reward: -1\n",
      "Episode 824/1000, Total Reward: -1\n",
      "Episode 825/1000, Total Reward: 1\n",
      "Episode 826/1000, Total Reward: 1\n",
      "Episode 827/1000, Total Reward: -1\n",
      "Episode 828/1000, Total Reward: -1\n",
      "Episode 829/1000, Total Reward: 1\n",
      "Episode 830/1000, Total Reward: 1\n",
      "Episode 831/1000, Total Reward: 1\n",
      "Episode 832/1000, Total Reward: -1\n",
      "Episode 833/1000, Total Reward: 1\n",
      "Episode 834/1000, Total Reward: 1\n",
      "Episode 835/1000, Total Reward: -1\n",
      "Episode 836/1000, Total Reward: -1\n",
      "Episode 837/1000, Total Reward: -1\n",
      "Episode 838/1000, Total Reward: 1\n",
      "Episode 839/1000, Total Reward: -1\n",
      "Episode 840/1000, Total Reward: 1\n",
      "Episode 841/1000, Total Reward: 1\n",
      "Episode 842/1000, Total Reward: 1\n",
      "Episode 843/1000, Total Reward: 1\n",
      "Episode 844/1000, Total Reward: -1\n",
      "Episode 845/1000, Total Reward: -1\n",
      "Episode 846/1000, Total Reward: -1\n",
      "Episode 847/1000, Total Reward: 1\n",
      "Episode 848/1000, Total Reward: 1\n",
      "Episode 849/1000, Total Reward: 1\n",
      "Episode 850/1000, Total Reward: 1\n",
      "Episode 851/1000, Total Reward: -1\n",
      "Episode 852/1000, Total Reward: -1\n",
      "Episode 853/1000, Total Reward: 1\n",
      "Episode 854/1000, Total Reward: -1\n",
      "Episode 855/1000, Total Reward: 1\n",
      "Episode 856/1000, Total Reward: 1\n",
      "Episode 857/1000, Total Reward: 1\n",
      "Episode 858/1000, Total Reward: 1\n",
      "Episode 859/1000, Total Reward: -1\n",
      "Episode 860/1000, Total Reward: -1\n",
      "Episode 861/1000, Total Reward: 1\n",
      "Episode 862/1000, Total Reward: -1\n",
      "Episode 863/1000, Total Reward: 1\n",
      "Episode 864/1000, Total Reward: 1\n",
      "Episode 865/1000, Total Reward: -1\n",
      "Episode 866/1000, Total Reward: 1\n",
      "Episode 867/1000, Total Reward: 1\n",
      "Episode 868/1000, Total Reward: 1\n",
      "Episode 869/1000, Total Reward: -1\n",
      "Episode 870/1000, Total Reward: 1\n",
      "Episode 871/1000, Total Reward: -1\n",
      "Episode 872/1000, Total Reward: 1\n",
      "Episode 873/1000, Total Reward: 1\n",
      "Episode 874/1000, Total Reward: 1\n",
      "Episode 875/1000, Total Reward: 1\n",
      "Episode 876/1000, Total Reward: 1\n",
      "Episode 877/1000, Total Reward: 1\n",
      "Episode 878/1000, Total Reward: 1\n",
      "Episode 879/1000, Total Reward: -1\n",
      "Episode 880/1000, Total Reward: 1\n",
      "Episode 881/1000, Total Reward: 1\n",
      "Episode 882/1000, Total Reward: -1\n",
      "Episode 883/1000, Total Reward: -1\n",
      "Episode 884/1000, Total Reward: -1\n",
      "Episode 885/1000, Total Reward: 1\n",
      "Episode 886/1000, Total Reward: -1\n",
      "Episode 887/1000, Total Reward: 1\n",
      "Episode 888/1000, Total Reward: -1\n",
      "Episode 889/1000, Total Reward: 1\n",
      "Episode 890/1000, Total Reward: 1\n",
      "Episode 891/1000, Total Reward: -1\n",
      "Episode 892/1000, Total Reward: -1\n",
      "Episode 893/1000, Total Reward: 1\n",
      "Episode 894/1000, Total Reward: 1\n",
      "Episode 895/1000, Total Reward: 1\n",
      "Episode 896/1000, Total Reward: 1\n",
      "Episode 897/1000, Total Reward: 1\n",
      "Episode 898/1000, Total Reward: 1\n",
      "Episode 899/1000, Total Reward: -1\n",
      "Episode 900/1000, Total Reward: -1\n",
      "Episode 901/1000, Total Reward: 1\n",
      "Episode 902/1000, Total Reward: 1\n",
      "Episode 903/1000, Total Reward: 1\n",
      "Episode 904/1000, Total Reward: 1\n",
      "Episode 905/1000, Total Reward: 1\n",
      "Episode 906/1000, Total Reward: 1\n",
      "Episode 907/1000, Total Reward: -1\n",
      "Episode 908/1000, Total Reward: 1\n",
      "Episode 909/1000, Total Reward: 1\n",
      "Episode 910/1000, Total Reward: 1\n",
      "Episode 911/1000, Total Reward: -1\n",
      "Episode 912/1000, Total Reward: -1\n",
      "Episode 913/1000, Total Reward: 1\n",
      "Episode 914/1000, Total Reward: 1\n",
      "Episode 915/1000, Total Reward: -1\n",
      "Episode 916/1000, Total Reward: 1\n",
      "Episode 917/1000, Total Reward: -1\n",
      "Episode 918/1000, Total Reward: 1\n",
      "Episode 919/1000, Total Reward: 1\n",
      "Episode 920/1000, Total Reward: 1\n",
      "Episode 921/1000, Total Reward: -1\n",
      "Episode 922/1000, Total Reward: 1\n",
      "Episode 923/1000, Total Reward: 1\n",
      "Episode 924/1000, Total Reward: -1\n",
      "Episode 925/1000, Total Reward: 1\n",
      "Episode 926/1000, Total Reward: 1\n",
      "Episode 927/1000, Total Reward: 1\n",
      "Episode 928/1000, Total Reward: 1\n",
      "Episode 929/1000, Total Reward: 1\n",
      "Episode 930/1000, Total Reward: 1\n",
      "Episode 931/1000, Total Reward: 1\n",
      "Episode 932/1000, Total Reward: 1\n",
      "Episode 933/1000, Total Reward: -1\n",
      "Episode 934/1000, Total Reward: 1\n",
      "Episode 935/1000, Total Reward: -1\n",
      "Episode 936/1000, Total Reward: 1\n",
      "Episode 937/1000, Total Reward: -1\n",
      "Episode 938/1000, Total Reward: 1\n",
      "Episode 939/1000, Total Reward: -1\n",
      "Episode 940/1000, Total Reward: -1\n",
      "Episode 941/1000, Total Reward: -1\n",
      "Episode 942/1000, Total Reward: 1\n",
      "Episode 943/1000, Total Reward: -1\n",
      "Episode 944/1000, Total Reward: 1\n",
      "Episode 945/1000, Total Reward: 1\n",
      "Episode 946/1000, Total Reward: -1\n",
      "Episode 947/1000, Total Reward: 1\n",
      "Episode 948/1000, Total Reward: 1\n",
      "Episode 949/1000, Total Reward: 1\n",
      "Episode 950/1000, Total Reward: -1\n",
      "Episode 951/1000, Total Reward: -1\n",
      "Episode 952/1000, Total Reward: 1\n",
      "Episode 953/1000, Total Reward: -1\n",
      "Episode 954/1000, Total Reward: -1\n",
      "Episode 955/1000, Total Reward: -1\n",
      "Episode 956/1000, Total Reward: -1\n",
      "Episode 957/1000, Total Reward: 0\n",
      "Episode 958/1000, Total Reward: 1\n",
      "Episode 959/1000, Total Reward: 1\n",
      "Episode 960/1000, Total Reward: -1\n",
      "Episode 961/1000, Total Reward: 1\n",
      "Episode 962/1000, Total Reward: 1\n",
      "Episode 963/1000, Total Reward: 1\n",
      "Episode 964/1000, Total Reward: 1\n",
      "Episode 965/1000, Total Reward: 1\n",
      "Episode 966/1000, Total Reward: 1\n",
      "Episode 967/1000, Total Reward: -1\n",
      "Episode 968/1000, Total Reward: 1\n",
      "Episode 969/1000, Total Reward: -1\n",
      "Episode 970/1000, Total Reward: -1\n",
      "Episode 971/1000, Total Reward: -1\n",
      "Episode 972/1000, Total Reward: 1\n",
      "Episode 973/1000, Total Reward: 1\n",
      "Episode 974/1000, Total Reward: 1\n",
      "Episode 975/1000, Total Reward: -1\n",
      "Episode 976/1000, Total Reward: 1\n",
      "Episode 977/1000, Total Reward: 1\n",
      "Episode 978/1000, Total Reward: 1\n",
      "Episode 979/1000, Total Reward: 1\n",
      "Episode 980/1000, Total Reward: 1\n",
      "Episode 981/1000, Total Reward: 1\n",
      "Episode 982/1000, Total Reward: -1\n",
      "Episode 983/1000, Total Reward: 1\n",
      "Episode 984/1000, Total Reward: 1\n",
      "Episode 985/1000, Total Reward: 1\n",
      "Episode 986/1000, Total Reward: 1\n",
      "Episode 987/1000, Total Reward: -1\n",
      "Episode 988/1000, Total Reward: 1\n",
      "Episode 989/1000, Total Reward: -1\n",
      "Episode 990/1000, Total Reward: 1\n",
      "Episode 991/1000, Total Reward: 1\n",
      "Episode 992/1000, Total Reward: -1\n",
      "Episode 993/1000, Total Reward: -1\n",
      "Episode 994/1000, Total Reward: 1\n",
      "Episode 995/1000, Total Reward: 1\n",
      "Episode 996/1000, Total Reward: 1\n",
      "Episode 997/1000, Total Reward: -1\n",
      "Episode 998/1000, Total Reward: -1\n",
      "Episode 999/1000, Total Reward: -1\n",
      "Episode 1000/1000, Total Reward: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the PPO Actor-Critic Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.actor = nn.Linear(128, action_size)\n",
    "        self.critic = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "\n",
    "# Enhanced PPO Agent\n",
    "class EnhancedPPOAgent:\n",
    "    def __init__(self, state_size, action_size, lr=1e-4, gamma=0.98, eps_clip=0.15):\n",
    "        self.model = PPOActorCritic(state_size, action_size).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        logits, _ = self.model(state)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        action = torch.multinomial(probabilities, 1).item()\n",
    "        return action, probabilities[action].item()\n",
    "\n",
    "    def evaluate_action(self, state, action):\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        logits, value = self.model(state)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        action_probs = probabilities.gather(1, action.unsqueeze(1)).squeeze()\n",
    "        return action_probs, value\n",
    "\n",
    "    def train(self, memory, batch_size):\n",
    "        # Prepare batches\n",
    "        states, actions, rewards, next_states, dones, old_probs = zip(*memory)\n",
    "        states = torch.stack([torch.tensor(s, dtype=torch.float32) for s in states]).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        old_probs = torch.tensor(old_probs, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        cumulative_rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, done in zip(reversed(rewards), reversed(dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + self.gamma * discounted_reward\n",
    "            cumulative_rewards.insert(0, discounted_reward)\n",
    "        cumulative_rewards = torch.tensor(cumulative_rewards, dtype=torch.float32).to(self.device)\n",
    "        advantages = (cumulative_rewards - cumulative_rewards.mean()) / (cumulative_rewards.std() + 1e-8)\n",
    "\n",
    "        for _ in range(4):  # Multiple epochs\n",
    "            new_probs, values = self.evaluate_action(states, actions)\n",
    "            ratio = (new_probs / old_probs).squeeze()\n",
    "\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), cumulative_rewards)\n",
    "            loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "# Training PPO Agent\n",
    "EPISODES = 1000\n",
    "BATCH_SIZE = 64\n",
    "rewards = []\n",
    "memory = []\n",
    "\n",
    "# Initialize Enhanced PPO Agent\n",
    "agent = EnhancedPPOAgent(state_size, action_size)\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.array(state[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "        action, prob = agent.act(state)\n",
    "        if action not in valid_actions:\n",
    "            action = random.choice(valid_actions)\n",
    "\n",
    "        opponent_action = random.choice(valid_actions)\n",
    "        next_step = env.step([action, opponent_action])\n",
    "        done = env.done\n",
    "        reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "        next_state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)\n",
    "\n",
    "        memory.append((state, action, reward, next_state, done, prob))\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            agent.train(memory, batch_size=BATCH_SIZE)\n",
    "            memory = []\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "    print(f\"Episode {episode + 1}/{EPISODES}, Total Reward: {total_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc049824-85da-4a23-ac64-a430df54c432",
   "metadata": {},
   "source": [
    "### Evaluation on Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a07d3ee-3d0b-4a05-bac9-e0dcaa2c67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced PPO Agent vs Random Agent: 0.4\n",
      "Enhanced PPO Agent vs Negamax Agent: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate(env_name, agent, opponent, num_episodes=10):\n",
    "\n",
    "    \n",
    "    env = make(env_name)\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.array(state[0]['observation'][\"board\"]).reshape(-1)  # Flatten the board\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Agent's action\n",
    "            action, _ = agent.act(state)\n",
    "            valid_actions = [c for c in range(env.configuration.columns) if state[c] == 0]\n",
    "            if action not in valid_actions:\n",
    "                action = random.choice(valid_actions)\n",
    "\n",
    "            # Opponent's action\n",
    "            if opponent == \"random\":\n",
    "                opponent_action = random.choice(valid_actions)\n",
    "            elif opponent == \"negamax\":\n",
    "                opponent_action = \"negamax\"\n",
    "            else:\n",
    "                raise ValueError(\"Invalid opponent specified.\")\n",
    "\n",
    "            # Step the environment\n",
    "            next_step = env.step([action, opponent_action])\n",
    "            done = env.done\n",
    "            reward = next_step[0]['reward'] if next_step[0]['reward'] is not None else 0\n",
    "            state = np.array(next_step[0]['observation'][\"board\"]).reshape(-1)\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "# Evaluate the trained Enhanced PPO Agent\n",
    "ppo_vs_random = evaluate(\"connectx\", agent, \"random\", num_episodes=10)\n",
    "ppo_vs_negamax = evaluate(\"connectx\", agent, \"negamax\", num_episodes=10)\n",
    "\n",
    "print(f\"Enhanced PPO Agent vs Random Agent: {ppo_vs_random}\")\n",
    "print(f\"Enhanced PPO Agent vs Negamax Agent: {ppo_vs_negamax}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32b775-3979-4d00-98ed-5ebcf249be61",
   "metadata": {},
   "source": [
    "# Analysis of Results:\n",
    "- The metrics \"Enhanced PPO Agent vs Random Agent: 0.4\" and \"Enhanced PPO Agent vs Negamax Agent: 0.0\" likely represent the performance scores or win rates of the Enhanced Proximal Policy Optimization (PPO) agent in games or simulations against two different types of opponents:\n",
    "\n",
    "1. Enhanced PPO Agent vs Random Agent (0.4): This suggests the Enhanced PPO Agent has a 40% win rate or a performance score of 0.4 when competing against a Random Agent, which likely selects moves randomly without strategy.\n",
    "\n",
    "2. A 0.4 score could indicate moderate success, as the Random Agent is typically not challenging.\n",
    "\n",
    "- Enhanced PPO Agent vs Negamax Agent (0.0): This score implies the Enhanced PPO Agent has a 0% win rate or no success against the Negamax Agent, a more strategic opponent using a game-theoretic algorithm often optimized for zero-sum games like chess or Connect Four. The 0.0 score highlights a significant difficulty for the Enhanced PPO Agent in competing with this advanced algorithm, possibly due to inadequate training or mismatched strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da384fb-5211-4695-9ee1-ef0a265114ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e79264a0-c2ef-4122-a3bb-13572b310ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This agent random chooses a non-empty column.\n",
    "def PPOAgent(observation, configuration):\n",
    "    from random import choice\n",
    "    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7adcfc-229c-4e0f-a53a-9b73f65f4a18",
   "metadata": {},
   "source": [
    "### Testing the PPO Agent as it outperformed others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a69aa439-c3e8-4a62-8086-a12fd462bc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"<!--\n",
       "  Copyright 2020 Kaggle Inc\n",
       "\n",
       "  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "  you may not use this file except in compliance with the License.\n",
       "  You may obtain a copy of the License at\n",
       "\n",
       "      http://www.apache.org/licenses/LICENSE-2.0\n",
       "\n",
       "  Unless required by applicable law or agreed to in writing, software\n",
       "  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "  See the License for the specific language governing permissions and\n",
       "  limitations under the License.\n",
       "-->\n",
       "<!DOCTYPE html>\n",
       "<html lang=&quot;en&quot;>\n",
       "  <head>\n",
       "    <title>Kaggle Simulation Player</title>\n",
       "    <meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; />\n",
       "    <link\n",
       "      rel=&quot;stylesheet&quot;\n",
       "      href=&quot;https://cdnjs.cloudflare.com/ajax/libs/meyer-reset/2.0/reset.css&quot;\n",
       "      crossorigin=&quot;anonymous&quot;\n",
       "    />\n",
       "    <style type=&quot;text/css&quot;>\n",
       "      html,\n",
       "      body {\n",
       "        height: 100%;\n",
       "        font-family: sans-serif;\n",
       "        margin: 0px;\n",
       "      }\n",
       "      canvas {\n",
       "        /* image-rendering: -moz-crisp-edges;\n",
       "        image-rendering: -webkit-crisp-edges;\n",
       "        image-rendering: pixelated;\n",
       "        image-rendering: crisp-edges; */\n",
       "      }\n",
       "    </style>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/dist/preact.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/preact@10.0.1/hooks/dist/hooks.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/htm@2.2.1/dist/htm.umd.js&quot;></script>\n",
       "    <script src=&quot;https://unpkg.com/chess.js@0.12.0/chess.js&quot;></script>\n",
       "    <script>\n",
       "      // Polyfill for Styled Components\n",
       "      window.React = {\n",
       "        ...preact,\n",
       "        createElement: preact.h,\n",
       "        PropTypes: { func: {} },\n",
       "      };\n",
       "    </script>\n",
       "    <script src=&quot;https://unpkg.com/styled-components@3.5.0-0/dist/styled-components.min.js&quot;></script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <script>\n",
       "      \n",
       "window.kaggle = {\n",
       "  &quot;debug&quot;: true,\n",
       "  &quot;playing&quot;: true,\n",
       "  &quot;step&quot;: 0,\n",
       "  &quot;controls&quot;: true,\n",
       "  &quot;environment&quot;: {\n",
       "    &quot;id&quot;: &quot;8e02b08c-b6be-11ef-8ce1-847b57b245e8&quot;,\n",
       "    &quot;name&quot;: &quot;connectx&quot;,\n",
       "    &quot;title&quot;: &quot;ConnectX&quot;,\n",
       "    &quot;description&quot;: &quot;Classic Connect in a row but configurable.&quot;,\n",
       "    &quot;version&quot;: &quot;1.0.1&quot;,\n",
       "    &quot;configuration&quot;: {\n",
       "      &quot;episodeSteps&quot;: 1000,\n",
       "      &quot;actTimeout&quot;: 2,\n",
       "      &quot;runTimeout&quot;: 1200,\n",
       "      &quot;columns&quot;: 7,\n",
       "      &quot;rows&quot;: 6,\n",
       "      &quot;inarow&quot;: 4,\n",
       "      &quot;agentTimeout&quot;: 60,\n",
       "      &quot;timeout&quot;: 2\n",
       "    },\n",
       "    &quot;specification&quot;: {\n",
       "      &quot;action&quot;: {\n",
       "        &quot;description&quot;: &quot;Column to drop a checker onto the board.&quot;,\n",
       "        &quot;type&quot;: &quot;integer&quot;,\n",
       "        &quot;minimum&quot;: 0,\n",
       "        &quot;default&quot;: 0\n",
       "      },\n",
       "      &quot;agents&quot;: [\n",
       "        2\n",
       "      ],\n",
       "      &quot;configuration&quot;: {\n",
       "        &quot;episodeSteps&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum number of steps in the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;minimum&quot;: 1,\n",
       "          &quot;default&quot;: 1000\n",
       "        },\n",
       "        &quot;actTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) to obtain an action from an agent.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 2\n",
       "        },\n",
       "        &quot;runTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Maximum runtime (seconds) of an episode (not necessarily DONE).&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 1200\n",
       "        },\n",
       "        &quot;columns&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of columns on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 7,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;rows&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of rows on the board&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 6,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;inarow&quot;: {\n",
       "          &quot;description&quot;: &quot;The number of checkers in a row required to win.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 4,\n",
       "          &quot;minimum&quot;: 1\n",
       "        },\n",
       "        &quot;agentTimeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete field kept for backwards compatibility, please use observation.remainingOverageTime.&quot;,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;timeout&quot;: {\n",
       "          &quot;description&quot;: &quot;Obsolete copy of actTimeout maintained for backwards compatibility. May be removed in the future.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;default&quot;: 2,\n",
       "          &quot;minimum&quot;: 0\n",
       "        }\n",
       "      },\n",
       "      &quot;info&quot;: {},\n",
       "      &quot;observation&quot;: {\n",
       "        &quot;remainingOverageTime&quot;: {\n",
       "          &quot;description&quot;: &quot;Total remaining banked time (seconds) that can be used in excess of per-step actTimeouts -- agent is disqualified with TIMEOUT status when this drops below 0.&quot;,\n",
       "          &quot;shared&quot;: false,\n",
       "          &quot;type&quot;: &quot;number&quot;,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 60\n",
       "        },\n",
       "        &quot;step&quot;: {\n",
       "          &quot;description&quot;: &quot;Current step within the episode.&quot;,\n",
       "          &quot;type&quot;: &quot;integer&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;minimum&quot;: 0,\n",
       "          &quot;default&quot;: 0\n",
       "        },\n",
       "        &quot;board&quot;: {\n",
       "          &quot;description&quot;: &quot;Serialized grid (rows x columns). 0 = Empty, 1 = P1, 2 = P2&quot;,\n",
       "          &quot;type&quot;: &quot;array&quot;,\n",
       "          &quot;shared&quot;: true,\n",
       "          &quot;default&quot;: []\n",
       "        },\n",
       "        &quot;mark&quot;: {\n",
       "          &quot;defaults&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ],\n",
       "          &quot;description&quot;: &quot;Which checkers are the agents.&quot;,\n",
       "          &quot;enum&quot;: [\n",
       "            1,\n",
       "            2\n",
       "          ]\n",
       "        }\n",
       "      },\n",
       "      &quot;reward&quot;: {\n",
       "        &quot;description&quot;: &quot;-1 = Lost, 0 = Draw/Ongoing, 1 = Won&quot;,\n",
       "        &quot;enum&quot;: [\n",
       "          -1,\n",
       "          0,\n",
       "          1\n",
       "        ],\n",
       "        &quot;default&quot;: 0,\n",
       "        &quot;type&quot;: [\n",
       "          &quot;number&quot;,\n",
       "          &quot;null&quot;\n",
       "        ]\n",
       "      }\n",
       "    },\n",
       "    &quot;steps&quot;: [\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 0,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 1,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 2,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 3,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 4,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 6,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 5,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 6,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 1,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 7,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 8,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 5,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 9,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 10,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 11,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 12,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 3,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 13,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 14,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;ACTIVE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 2,\n",
       "          &quot;reward&quot;: 0,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;INACTIVE&quot;\n",
       "        }\n",
       "      ],\n",
       "      [\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: 1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;step&quot;: 15,\n",
       "            &quot;board&quot;: [\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              2,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              0,\n",
       "              1,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2,\n",
       "              1,\n",
       "              2,\n",
       "              0,\n",
       "              1,\n",
       "              2\n",
       "            ],\n",
       "            &quot;mark&quot;: 1\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        },\n",
       "        {\n",
       "          &quot;action&quot;: 0,\n",
       "          &quot;reward&quot;: -1,\n",
       "          &quot;info&quot;: {},\n",
       "          &quot;observation&quot;: {\n",
       "            &quot;remainingOverageTime&quot;: 60,\n",
       "            &quot;mark&quot;: 2\n",
       "          },\n",
       "          &quot;status&quot;: &quot;DONE&quot;\n",
       "        }\n",
       "      ]\n",
       "    ],\n",
       "    &quot;rewards&quot;: [\n",
       "      1,\n",
       "      -1\n",
       "    ],\n",
       "    &quot;statuses&quot;: [\n",
       "      &quot;DONE&quot;,\n",
       "      &quot;DONE&quot;\n",
       "    ],\n",
       "    &quot;schema_version&quot;: 1,\n",
       "    &quot;info&quot;: {}\n",
       "  },\n",
       "  &quot;logs&quot;: [\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 5.7e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000482,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 7.4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.5e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.6e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [],\n",
       "    [],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000147,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.8e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 7.3e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 0.000172,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 2.4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.7e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3.4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 4.9e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 5.4e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 2.7e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ],\n",
       "    [\n",
       "      {},\n",
       "      {\n",
       "        &quot;duration&quot;: 3.1e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      }\n",
       "    ],\n",
       "    [\n",
       "      {\n",
       "        &quot;duration&quot;: 3.7e-05,\n",
       "        &quot;stdout&quot;: &quot;&quot;,\n",
       "        &quot;stderr&quot;: &quot;&quot;\n",
       "      },\n",
       "      {}\n",
       "    ]\n",
       "  ],\n",
       "  &quot;mode&quot;: &quot;ipython&quot;,\n",
       "  &quot;width&quot;: 500,\n",
       "  &quot;height&quot;: 450\n",
       "};\n",
       "\n",
       "\n",
       "window.kaggle.renderer = // Copyright 2020 Kaggle Inc\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "function renderer({\n",
       "  act,\n",
       "  agents,\n",
       "  environment,\n",
       "  frame,\n",
       "  height = 400,\n",
       "  interactive,\n",
       "  isInteractive,\n",
       "  parent,\n",
       "  step,\n",
       "  update,\n",
       "  width = 400,\n",
       "}) {\n",
       "  // Configuration.\n",
       "  const { rows, columns, inarow } = environment.configuration;\n",
       "\n",
       "  // Common Dimensions.\n",
       "  const unit = 8;\n",
       "  const minCanvasSize = Math.min(height, width);\n",
       "  const minOffset = minCanvasSize > 400 ? 30 : unit / 2;\n",
       "  const cellSize = Math.min(\n",
       "    (width - minOffset * 2) / columns,\n",
       "    (height - minOffset * 2) / rows\n",
       "  );\n",
       "  const cellInset = 0.8;\n",
       "  const pieceScale = cellSize / 100;\n",
       "  const xOffset = Math.max(0, (width - cellSize * columns) / 2);\n",
       "  const yOffset = Math.max(0, (height - cellSize * rows) / 2);\n",
       "\n",
       "  // Canvas Setup.\n",
       "  let canvas = parent.querySelector(&quot;canvas&quot;);\n",
       "  if (!canvas) {\n",
       "    canvas = document.createElement(&quot;canvas&quot;);\n",
       "    parent.appendChild(canvas);\n",
       "\n",
       "    if (interactive) {\n",
       "      canvas.addEventListener(&quot;click&quot;, evt => {\n",
       "        if (!isInteractive()) return;\n",
       "        const rect = evt.target.getBoundingClientRect();\n",
       "        const col = Math.floor((evt.clientX - rect.left - xOffset) / cellSize);\n",
       "        if (col >= 0 && col < columns) act(col);\n",
       "      });\n",
       "    }\n",
       "  }\n",
       "  canvas.style.cursor = isInteractive() ? &quot;pointer&quot; : &quot;default&quot;;\n",
       "\n",
       "  // Character Paths (based on 100x100 tiles).\n",
       "  const kPath = new Path2D(\n",
       "    `M78.3,96.5c-0.1,0.4-0.5,0.6-1.1,0.6H64.9c-0.7,0-1.4-0.3-1.9-1l-20.3-26L37,75.5v20.1 c0,0.9-0.5,1.4-1.4,1.4H26c-0.9,0-1.4-0.5-1.4-1.4V3.9c0-0.9,0.5-1.4,1.4-1.4h9.5C36.5,2.5,37,3,37,3.9v56.5l24.3-24.7 c0.6-0.6,1.3-1,1.9-1H76c0.6,0,0.9,0.2,1.1,0.7c0.2,0.6,0.1,1-0.1,1.2l-25.7,25L78,95.1C78.4,95.5,78.5,95.9,78.3,96.5z`\n",
       "  );\n",
       "  const goose1Path = new Path2D(\n",
       "    `M8.8,92.7c-4-18.5,4.7-37.2,20.7-46.2c0,0,2.7-1.4,3.4-1.9c2.2-1.6,3-2.1,3-5c0-5-2.1-7.2-2.1-7.2 c-3.9-3.3-6.3-8.2-6.3-13.7c0-10,8.1-18.1,18.1-18.1s18.1,8.1,18.1,18.1c0,6-1.5,32.7-2.3,38.8l-0.1,1`\n",
       "  );\n",
       "  const goose2Path = new Path2D(\n",
       "    `M27.4,19L8.2,27.6c0,0-7.3,2.9,2.6,5c6.1,1.3,24,5.9,24,5.9l1,0.3`\n",
       "  );\n",
       "  const goose3Path = new Path2D(\n",
       "    `M63.7,99.6C52.3,99.6,43,90.3,43,78.9s9.3-20.7,20.7-20.7c10.6,0,34.4,0.1,35.8,9`\n",
       "  );\n",
       "\n",
       "  // Canvas setup and reset.\n",
       "  let c = canvas.getContext(&quot;2d&quot;);\n",
       "  canvas.width = width;\n",
       "  canvas.height = height;\n",
       "  c.fillStyle = &quot;#000B2A&quot;;\n",
       "  c.fillRect(0, 0, canvas.width, canvas.height);\n",
       "\n",
       "  const getRowCol = cell => [Math.floor(cell / columns), cell % columns];\n",
       "\n",
       "  const getColor = (mark, opacity = 1) => {\n",
       "    if (mark === 1) return `rgba(0,255,255,${opacity})`;\n",
       "    if (mark === 2) return `rgba(255,255,255,${opacity})`;\n",
       "    return &quot;#fff&quot;;\n",
       "  };\n",
       "\n",
       "  const drawCellCircle = (cell, xFrame = 1, yFrame = 1, radiusOffset = 0) => {\n",
       "    const [row, col] = getRowCol(cell);\n",
       "    c.arc(\n",
       "      xOffset + xFrame * (col * cellSize + cellSize / 2),\n",
       "      yOffset + yFrame * (row * cellSize + cellSize / 2),\n",
       "      (cellInset * cellSize) / 2 - radiusOffset,\n",
       "      2 * Math.PI,\n",
       "      false\n",
       "    );\n",
       "  };\n",
       "\n",
       "  // Render the pieces.\n",
       "  const board = environment.steps[step][0].observation.board;\n",
       "\n",
       "  const drawPiece = mark => {\n",
       "    // Base Styles.\n",
       "    const opacity = minCanvasSize < 300 ? 0.6 - minCanvasSize / 1000 : 0.1;\n",
       "    c.fillStyle = getColor(mark, opacity);\n",
       "    c.strokeStyle = getColor(mark);\n",
       "    c.shadowColor = getColor(mark);\n",
       "    c.shadowBlur = 8 / cellInset;\n",
       "    c.lineWidth = 1 / cellInset;\n",
       "\n",
       "    // Outer circle.\n",
       "    c.save();\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 50, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.lineWidth *= 4;\n",
       "    c.stroke();\n",
       "    c.fill();\n",
       "    c.restore();\n",
       "\n",
       "    // Inner circle.\n",
       "    c.beginPath();\n",
       "    c.arc(50, 50, 40, 2 * Math.PI, false);\n",
       "    c.closePath();\n",
       "    c.stroke();\n",
       "\n",
       "    // Kaggle &quot;K&quot;.\n",
       "    if (mark === 1) {\n",
       "      const scale = 0.54;\n",
       "      c.save();\n",
       "      c.translate(23, 23);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(kPath);\n",
       "      c.restore();\n",
       "    }\n",
       "\n",
       "    // Kaggle &quot;Goose&quot;.\n",
       "    if (mark === 2) {\n",
       "      const scale = 0.6;\n",
       "      c.save();\n",
       "      c.translate(24, 28);\n",
       "      c.scale(scale, scale);\n",
       "      c.lineWidth /= scale;\n",
       "      c.shadowBlur /= scale;\n",
       "      c.stroke(goose1Path);\n",
       "      c.stroke(goose2Path);\n",
       "      c.stroke(goose3Path);\n",
       "      c.beginPath();\n",
       "      c.arc(38.5, 18.6, 2.7, 0, Math.PI * 2, false);\n",
       "      c.closePath();\n",
       "      c.fill();\n",
       "      c.restore();\n",
       "    }\n",
       "  };\n",
       "\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    const [row, col] = getRowCol(i);\n",
       "    if (board[i] === 0) continue;\n",
       "    // Easing In.\n",
       "    let yFrame = Math.min(\n",
       "      (columns * Math.pow(frame, 3)) / Math.floor(i / columns),\n",
       "      1\n",
       "    );\n",
       "\n",
       "    if (\n",
       "      step > 1 &&\n",
       "      environment.steps[step - 1][0].observation.board[i] === board[i]\n",
       "    ) {\n",
       "      yFrame = 1;\n",
       "    }\n",
       "\n",
       "    c.save();\n",
       "    c.translate(\n",
       "      xOffset + cellSize * col + (cellSize - cellSize * cellInset) / 2,\n",
       "      yOffset +\n",
       "        yFrame * (cellSize * row) +\n",
       "        (cellSize - cellSize * cellInset) / 2\n",
       "    );\n",
       "    c.scale(pieceScale * cellInset, pieceScale * cellInset);\n",
       "    drawPiece(board[i]);\n",
       "    c.restore();\n",
       "  }\n",
       "\n",
       "  // Background Gradient.\n",
       "  const bgRadius = (Math.min(rows, columns) * cellSize) / 2;\n",
       "  const bgStyle = c.createRadialGradient(\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    0,\n",
       "    xOffset + (cellSize * columns) / 2,\n",
       "    yOffset + (cellSize * rows) / 2,\n",
       "    bgRadius\n",
       "  );\n",
       "  bgStyle.addColorStop(0, &quot;#000B49&quot;);\n",
       "  bgStyle.addColorStop(1, &quot;#000B2A&quot;);\n",
       "\n",
       "  // Render the board overlay.\n",
       "  c.beginPath();\n",
       "  c.rect(0, 0, canvas.width, canvas.height);\n",
       "  c.closePath();\n",
       "  c.shadowBlur = 0;\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    drawCellCircle(i);\n",
       "    c.closePath();\n",
       "  }\n",
       "  c.fillStyle = bgStyle;\n",
       "  c.fill(&quot;evenodd&quot;);\n",
       "\n",
       "  // Render the board overlay cell outlines.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    c.beginPath();\n",
       "    drawCellCircle(i);\n",
       "    c.strokeStyle = &quot;#0361B2&quot;;\n",
       "    c.lineWidth = 1;\n",
       "    c.stroke();\n",
       "    c.closePath();\n",
       "  }\n",
       "\n",
       "  const drawLine = (fromCell, toCell) => {\n",
       "    if (frame < 0.5) return;\n",
       "    const lineFrame = (frame - 0.5) / 0.5;\n",
       "    const x1 = xOffset + (fromCell % columns) * cellSize + cellSize / 2;\n",
       "    const x2 =\n",
       "      x1 +\n",
       "      lineFrame *\n",
       "        (xOffset + ((toCell % columns) * cellSize + cellSize / 2) - x1);\n",
       "    const y1 =\n",
       "      yOffset + Math.floor(fromCell / columns) * cellSize + cellSize / 2;\n",
       "    const y2 =\n",
       "      y1 +\n",
       "      lineFrame *\n",
       "        (yOffset + Math.floor(toCell / columns) * cellSize + cellSize / 2 - y1);\n",
       "    c.beginPath();\n",
       "    c.lineCap = &quot;round&quot;;\n",
       "    c.lineWidth = 4;\n",
       "    c.strokeStyle = getColor(board[fromCell]);\n",
       "    c.shadowBlur = 8;\n",
       "    c.shadowColor = getColor(board[fromCell]);\n",
       "    c.moveTo(x1, y1);\n",
       "    c.lineTo(x2, y2);\n",
       "    c.stroke();\n",
       "  };\n",
       "\n",
       "  // Generate a graph of the board.\n",
       "  const getCell = (cell, rowOffset, columnOffset) => {\n",
       "    const row = Math.floor(cell / columns) + rowOffset;\n",
       "    const col = (cell % columns) + columnOffset;\n",
       "    if (row < 0 || row >= rows || col < 0 || col >= columns) return -1;\n",
       "    return col + row * columns;\n",
       "  };\n",
       "  const makeNode = cell => {\n",
       "    const node = { cell, directions: [], value: board[cell] };\n",
       "    for (let r = -1; r <= 1; r++) {\n",
       "      for (let c = -1; c <= 1; c++) {\n",
       "        if (r === 0 && c === 0) continue;\n",
       "        node.directions.push(getCell(cell, r, c));\n",
       "      }\n",
       "    }\n",
       "    return node;\n",
       "  };\n",
       "  const graph = board.map((_, i) => makeNode(i));\n",
       "\n",
       "  // Check for any wins!\n",
       "  const getSequence = (node, direction) => {\n",
       "    const sequence = [node.cell];\n",
       "    while (sequence.length < inarow) {\n",
       "      const next = graph[node.directions[direction]];\n",
       "      if (!next || node.value !== next.value || next.value === 0) return;\n",
       "      node = next;\n",
       "      sequence.push(node.cell);\n",
       "    }\n",
       "    return sequence;\n",
       "  };\n",
       "\n",
       "  // Check all nodes.\n",
       "  for (let i = 0; i < board.length; i++) {\n",
       "    // Check all directions (not the most efficient).\n",
       "    for (let d = 0; d < 8; d++) {\n",
       "      const seq = getSequence(graph[i], d);\n",
       "      if (seq) {\n",
       "        drawLine(seq[0], seq[inarow - 1]);\n",
       "        i = board.length;\n",
       "        break;\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "\n",
       "  // Upgrade the legend.\n",
       "  if (agents.length && (!agents[0].color || !agents[0].image)) {\n",
       "    const getPieceImage = mark => {\n",
       "      const pieceCanvas = document.createElement(&quot;canvas&quot;);\n",
       "      parent.appendChild(pieceCanvas);\n",
       "      pieceCanvas.style.marginLeft = &quot;10000px&quot;;\n",
       "      pieceCanvas.width = 100;\n",
       "      pieceCanvas.height = 100;\n",
       "      c = pieceCanvas.getContext(&quot;2d&quot;);\n",
       "      c.translate(10, 10);\n",
       "      c.scale(0.8, 0.8);\n",
       "      drawPiece(mark);\n",
       "      const dataUrl = pieceCanvas.toDataURL();\n",
       "      parent.removeChild(pieceCanvas);\n",
       "      return dataUrl;\n",
       "    };\n",
       "\n",
       "    agents.forEach(agent => {\n",
       "      agent.color = getColor(agent.index + 1);\n",
       "      agent.image = getPieceImage(agent.index + 1);\n",
       "    });\n",
       "    update({ agents });\n",
       "  }\n",
       "};\n",
       "\n",
       "\n",
       "    \n",
       "    </script>\n",
       "    <script>\n",
       "      const h = htm.bind(preact.h);\n",
       "      const { useContext, useEffect, useRef, useState } = preactHooks;\n",
       "      const styled = window.styled.default;\n",
       "\n",
       "      const Context = preact.createContext({});\n",
       "\n",
       "      const Loading = styled.div`\n",
       "        animation: rotate360 1.1s infinite linear;\n",
       "        border: 8px solid rgba(255, 255, 255, 0.2);\n",
       "        border-left-color: #0cb1ed;\n",
       "        border-radius: 50%;\n",
       "        height: 40px;\n",
       "        position: relative;\n",
       "        transform: translateZ(0);\n",
       "        width: 40px;\n",
       "\n",
       "        @keyframes rotate360 {\n",
       "          0% {\n",
       "            transform: rotate(0deg);\n",
       "          }\n",
       "          100% {\n",
       "            transform: rotate(360deg);\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Logo = styled(\n",
       "        (props) => h`\n",
       "        <a href=&quot;https://kaggle.com&quot; target=&quot;_blank&quot; className=${props.className}>\n",
       "          <svg width=&quot;62px&quot; height=&quot;20px&quot; viewBox=&quot;0 0 62 24&quot; version=&quot;1.1&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;>\n",
       "            <g fill=&quot;#1EBEFF&quot; fill-rule=&quot;nonzero&quot;>\n",
       "              <path d=&quot;M10.2,17.8c0,0.1-0.1,0.1-0.2,0.1H7.7c-0.1,0-0.3-0.1-0.4-0.2l-3.8-4.9l-1.1,1v3.8 c0,0.2-0.1,0.3-0.3,0.3H0.3c-0.2,0-0.3-0.1-0.3-0.3V0.3C0.1,0.1,0.2,0,0.3,0h1.8c0.2,0,0.3,0.1,0.3,0.3V11L7,6.3 c0.1-0.1,0.2-0.2,0.4-0.2h2.4c0.1,0,0.2,0,0.2,0.1c0,0.1,0,0.2,0,0.2l-4.9,4.7l5.1,6.3C10.2,17.6,10.2,17.7,10.2,17.8z&quot;/>\n",
       "              <path d=&quot;M19.6,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3v-0.4c-0.8,0.6-1.8,0.9-3,0.9c-1.1,0-2-0.3-2.8-1 c-0.8-0.7-1.2-1.6-1.2-2.7c0-1.7,1.1-2.9,3.2-3.5c0.8-0.2,2.1-0.5,3.8-0.6c0.1-0.6-0.1-1.2-0.5-1.7c-0.4-0.5-1-0.7-1.7-0.7 c-1,0-2,0.4-3,1C12.2,9.1,12.1,9.1,12,9l-0.9-1.3C11,7.5,11,7.4,11.1,7.3c1.3-0.9,2.7-1.4,4.2-1.4c1.1,0,2.1,0.3,2.8,0.8 c1.1,0.8,1.7,2,1.7,3.7v7.3C19.9,17.8,19.8,17.9,19.6,17.9z M17.5,12.4c-1.7,0.2-2.9,0.4-3.5,0.7c-0.9,0.4-1.2,0.9-1.1,1.6 c0.1,0.4,0.2,0.7,0.6,0.9c0.3,0.2,0.7,0.4,1.1,0.4c1.2,0.1,2.2-0.2,2.9-1V12.4z&quot;/>\n",
       "              <path d=&quot;M30.6,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3v11.7C32,20,31.5,21.5,30.6,22.5z M29.7,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7V9.9z&quot;/>\n",
       "              <path d=&quot;M42.9,22.5c-0.9,1-2.3,1.5-4,1.5c-1,0-2-0.3-2.9-0.8c-0.2-0.1-0.4-0.3-0.7-0.5 c-0.3-0.2-0.6-0.5-0.9-0.7c-0.1-0.1-0.1-0.2,0-0.4l1.2-1.2c0.1-0.1,0.1-0.1,0.2-0.1c0.1,0,0.1,0,0.2,0.1c1,1,1.9,1.5,2.8,1.5 c2.1,0,3.2-1.1,3.2-3.3v-1.4c-0.8,0.7-1.9,1-3.3,1c-1.7,0-3-0.6-4-1.9c-0.8-1.1-1.3-2.5-1.3-4.2c0-1.6,0.4-3,1.2-4.1 c0.9-1.3,2.3-2,4-2c1.3,0,2.4,0.3,3.3,1V6.4c0-0.2,0.1-0.3,0.3-0.3H44c0.2,0,0.3,0.1,0.3,0.3v11.7C44.3,20,43.8,21.5,42.9,22.5z M42,9.9c-0.4-1.1-1.4-1.7-3-1.7c-2,0-3.1,1.3-3.1,3.8c0,1.4,0.3,2.4,1,3.1c0.5,0.5,1.2,0.8,2,0.8c1.6,0,2.7-0.6,3.1-1.7L42,9.9 L42,9.9z&quot;/>\n",
       "              <path d=&quot;M48.3,17.9h-1.8c-0.2,0-0.3-0.1-0.3-0.3V0.3c0-0.2,0.1-0.3,0.3-0.3h1.8c0.2,0,0.3,0.1,0.3,0.3 v17.3C48.5,17.8,48.5,17.9,48.3,17.9z&quot;/>\n",
       "              <path d=&quot;M61.4,12.6c0,0.2-0.1,0.3-0.3,0.3h-8.5c0.1,0.9,0.5,1.6,1.1,2.2c0.7,0.6,1.6,0.9,2.7,0.9 c1,0,1.8-0.3,2.6-0.8c0.2-0.1,0.3-0.1,0.4,0l1.2,1.3c0.1,0.1,0.1,0.3,0,0.4c-1.3,0.9-2.7,1.4-4.4,1.4c-1.8,0-3.3-0.6-4.4-1.8 c-1.1-1.2-1.7-2.7-1.7-4.5c0-1.7,0.6-3.2,1.7-4.4c1-1.1,2.4-1.6,4.1-1.6c1.6,0,2.9,0.6,4,1.7c1.1,1.2,1.6,2.6,1.5,4.4L61.4,12.6 z M58,8.7c-0.6-0.5-1.3-0.8-2.1-0.8c-0.8,0-1.5,0.3-2.1,0.8c-0.6,0.5-1,1.2-1.1,2H59C59,9.9,58.6,9.3,58,8.7z&quot;/>\n",
       "            </g>\n",
       "          </svg>\n",
       "        </a>\n",
       "      `\n",
       "      )`\n",
       "        display: inline-flex;\n",
       "      `;\n",
       "\n",
       "      const Header = styled((props) => {\n",
       "        const { environment } = useContext(Context);\n",
       "\n",
       "        return h`<div className=${props.className} >\n",
       "          <${Logo} />\n",
       "          <span><b>Left / Right Arrow:</b> Increase / Decrease Step</span><span><b>0-9 Row Keys:</b> Playback Speed</span><span><b>Space:</b> Pause / Play</span>\n",
       "          ${environment.title}\n",
       "        </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-bottom: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        color: #fff;\n",
       "        display: flex;\n",
       "        flex: 0 0 36px;\n",
       "        font-size: 14px;\n",
       "        justify-content: space-between;\n",
       "        padding: 0 8px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Renderer = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { animate, debug, playing, renderer, speed } = context;\n",
       "        const ref = preact.createRef();\n",
       "\n",
       "        useEffect(async () => {\n",
       "          if (!ref.current) return;\n",
       "\n",
       "          const renderFrame = async (start, step, lastFrame) => {\n",
       "            if (step !== context.step) return;\n",
       "            if (lastFrame === 1) {\n",
       "              if (!animate) return;\n",
       "              start = Date.now();\n",
       "            }\n",
       "            const frame =\n",
       "              playing || animate\n",
       "                ? Math.min((Date.now() - start) / speed, 1)\n",
       "                : 1;\n",
       "            try {\n",
       "              if (debug) console.time(&quot;render&quot;);\n",
       "              await renderer({\n",
       "                ...context,\n",
       "                frame,\n",
       "                height: ref.current.clientHeight,\n",
       "                hooks: preactHooks,\n",
       "                parent: ref.current,\n",
       "                preact,\n",
       "                styled,\n",
       "                width: ref.current.clientWidth,\n",
       "              });\n",
       "            } catch (error) {\n",
       "              if (debug) console.error(error);\n",
       "              console.log({ ...context, frame, error });\n",
       "            } finally {\n",
       "              if (debug) console.timeEnd(&quot;render&quot;);\n",
       "            }\n",
       "            window.requestAnimationFrame(() => renderFrame(start, step, frame));\n",
       "          };\n",
       "\n",
       "          await renderFrame(Date.now(), context.step);\n",
       "        }, [ref.current, context.step, context.renderer]);\n",
       "\n",
       "        return h`<div className=${props.className} ref=${ref} />`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        height: 100%;\n",
       "        left: 0;\n",
       "        justify-content: center;\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Processing = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        const text = processing === true ? &quot;Processing...&quot; : processing;\n",
       "        return h`<div className=${props.className}>${text}</div>`;\n",
       "      })`\n",
       "        bottom: 0;\n",
       "        color: #fff;\n",
       "        font-size: 12px;\n",
       "        left: 0;\n",
       "        line-height: 24px;\n",
       "        position: absolute;\n",
       "        text-align: center;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Viewer = styled((props) => {\n",
       "        const { processing } = useContext(Context);\n",
       "        return h`<div className=${props.className}>\n",
       "          <${Renderer} />\n",
       "          ${processing && h`<${Processing} />`}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        background-image: radial-gradient(\n",
       "          circle closest-side,\n",
       "          #000b49,\n",
       "          #000b2a\n",
       "        );\n",
       "        display: flex;\n",
       "        flex: 1;\n",
       "        overflow: hidden;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      // Partitions the elements of arr into subarrays of max length num.\n",
       "      const groupIntoSets = (arr, num) => {\n",
       "        const sets = [];\n",
       "        arr.forEach(a => {\n",
       "          if (sets.length === 0 || sets[sets.length - 1].length === num) {\n",
       "            sets.push([]);\n",
       "          }\n",
       "          sets[sets.length - 1].push(a);\n",
       "        });\n",
       "        return sets;\n",
       "      }\n",
       "\n",
       "      // Expects `width` input prop to set proper max-width for agent name span.\n",
       "      const Legend = styled((props) => {\n",
       "        const { agents, legend } = useContext(Context);\n",
       "\n",
       "        const agentPairs = groupIntoSets(agents.sort((a, b) => a.index - b.index), 2);\n",
       "\n",
       "        return h`<div className=${props.className}>\n",
       "          ${agentPairs.map(agentList =>\n",
       "            h`<ul>\n",
       "                ${agentList.map(a =>\n",
       "                  h`<li key=${a.id} title=&quot;id: ${a.id}&quot; style=&quot;color:${a.color || &quot;#FFF&quot;}&quot;>\n",
       "                      ${a.image && h`<img src=${a.image} />`}\n",
       "                      <span>${a.name}</span>\n",
       "                    </li>`\n",
       "                )}\n",
       "              </ul>`)}\n",
       "        </div>`;\n",
       "      })`\n",
       "        background-color: #000b2a;\n",
       "        font-family: sans-serif;\n",
       "        font-size: 14px;\n",
       "        height: 48px;\n",
       "        width: 100%;\n",
       "\n",
       "        ul {\n",
       "          align-items: center;\n",
       "          display: flex;\n",
       "          flex-direction: row;\n",
       "          justify-content: center;\n",
       "        }\n",
       "\n",
       "        li {\n",
       "          align-items: center;\n",
       "          display: inline-flex;\n",
       "          transition: color 1s;\n",
       "        }\n",
       "\n",
       "        span {\n",
       "          max-width: ${p => (p.width || 400) * 0.5 - 36}px;\n",
       "          overflow: hidden;\n",
       "          text-overflow: ellipsis;\n",
       "          white-space: nowrap;\n",
       "        }\n",
       "\n",
       "        img {\n",
       "          height: 24px;\n",
       "          margin-left: 4px;\n",
       "          margin-right: 4px;\n",
       "          width: 24px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepInput = styled.input.attrs({\n",
       "        type: &quot;range&quot;,\n",
       "      })`\n",
       "        appearance: none;\n",
       "        background: rgba(255, 255, 255, 0.15);\n",
       "        border-radius: 2px;\n",
       "        display: block;\n",
       "        flex: 1;\n",
       "        height: 4px;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "        width: 100%;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "\n",
       "        &::-webkit-slider-thumb {\n",
       "          appearance: none;\n",
       "          background: #1ebeff;\n",
       "          border-radius: 100%;\n",
       "          cursor: pointer;\n",
       "          height: 12px;\n",
       "          margin: 0;\n",
       "          position: relative;\n",
       "          width: 12px;\n",
       "\n",
       "          &::after {\n",
       "            content: &quot;&quot;;\n",
       "            position: absolute;\n",
       "            top: 0px;\n",
       "            left: 0px;\n",
       "            width: 200px;\n",
       "            height: 8px;\n",
       "            background: green;\n",
       "          }\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const PlayButton = styled.button`\n",
       "        align-items: center;\n",
       "        background: none;\n",
       "        border: none;\n",
       "        color: white;\n",
       "        cursor: pointer;\n",
       "        display: flex;\n",
       "        flex: 0 0 56px;\n",
       "        font-size: 20px;\n",
       "        height: 40px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        outline: none;\n",
       "        transition: opacity 0.2s;\n",
       "\n",
       "        &:hover {\n",
       "          opacity: 1;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const StepCount = styled.span`\n",
       "        align-items: center;\n",
       "        color: white;\n",
       "        display: flex;\n",
       "        font-size: 14px;\n",
       "        justify-content: center;\n",
       "        opacity: 0.8;\n",
       "        padding: 0 16px;\n",
       "        pointer-events: none;\n",
       "      `;\n",
       "\n",
       "      const Controls = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "        const value = step + 1;\n",
       "        const onClick = () => (playing ? pause() : play());\n",
       "        const onInput = (e) => {\n",
       "          pause();\n",
       "          setStep(parseInt(e.target.value) - 1);\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${PlayButton} onClick=${onClick}><svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;24px&quot; height=&quot;24px&quot; viewBox=&quot;0 0 24 24&quot; fill=&quot;#FFFFFF&quot;>${\n",
       "          playing\n",
       "            ? h`<path d=&quot;M6 19h4V5H6v14zm8-14v14h4V5h-4z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "            : h`<path d=&quot;M8 5v14l11-7z&quot;/><path d=&quot;M0 0h24v24H0z&quot; fill=&quot;none&quot;/>`\n",
       "        }</svg><//>\n",
       "            <${StepInput} min=&quot;1&quot; max=${\n",
       "          environment.steps.length\n",
       "        } value=&quot;${value}&quot; onInput=${onInput} />\n",
       "            <${StepCount}>${value} / ${environment.steps.length}<//>\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        border-top: 4px solid #212121;\n",
       "        display: flex;\n",
       "        flex: 0 0 44px;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const Info = styled((props) => {\n",
       "        const {\n",
       "          environment,\n",
       "          playing,\n",
       "          step,\n",
       "          speed,\n",
       "          animate,\n",
       "          header,\n",
       "          controls,\n",
       "          settings,\n",
       "        } = useContext(Context);\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            info:\n",
       "            step(${step}),\n",
       "            playing(${playing ? &quot;T&quot; : &quot;F&quot;}),\n",
       "            speed(${speed}),\n",
       "            animate(${animate ? &quot;T&quot; : &quot;F&quot;})\n",
       "          </div>`;\n",
       "      })`\n",
       "        color: #888;\n",
       "        font-family: monospace;\n",
       "        font-size: 12px;\n",
       "      `;\n",
       "\n",
       "      const Settings = styled((props) => {\n",
       "        const { environment, pause, play, playing, setStep, step } = useContext(\n",
       "          Context\n",
       "        );\n",
       "\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            <${Info} />\n",
       "          </div>\n",
       "        `;\n",
       "      })`\n",
       "        background: #fff;\n",
       "        border-top: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        padding: 20px;\n",
       "        width: 100%;\n",
       "\n",
       "        h1 {\n",
       "          font-size: 20px;\n",
       "        }\n",
       "      `;\n",
       "\n",
       "      const Player = styled((props) => {\n",
       "        const context = useContext(Context);\n",
       "        const { agents, controls, header, legend, loading, settings, width } = context;\n",
       "        return h`\n",
       "          <div className=${props.className}>\n",
       "            ${loading && h`<${Loading} />`}\n",
       "            ${!loading && header && h`<${Header} />`}\n",
       "            ${!loading && h`<${Viewer} />`}\n",
       "            ${!loading && legend && h`<${Legend} width=${width}/>`}\n",
       "            ${!loading && controls && h`<${Controls} />`}\n",
       "            ${!loading && settings && h`<${Settings} />`}\n",
       "          </div>`;\n",
       "      })`\n",
       "        align-items: center;\n",
       "        background: #212121;\n",
       "        border: 4px solid #212121;\n",
       "        box-sizing: border-box;\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        height: 100%;\n",
       "        justify-content: center;\n",
       "        position: relative;\n",
       "        width: 100%;\n",
       "      `;\n",
       "\n",
       "      const App = () => {\n",
       "        const renderCountRef = useRef(0);\n",
       "        const [_, setRenderCount] = useState(0);\n",
       "\n",
       "        // These are bindings to the 0-9 keys and are milliseconds of timeout per step\n",
       "        const speeds = [\n",
       "          0,\n",
       "          3000,\n",
       "          1000,\n",
       "          500,\n",
       "          333, // Default\n",
       "          200,\n",
       "          100,\n",
       "          50,\n",
       "          25,\n",
       "          10,\n",
       "        ];\n",
       "\n",
       "        const contextRef = useRef({\n",
       "          animate: false,\n",
       "          agents: [],\n",
       "          controls: false,\n",
       "          debug: false,\n",
       "          environment: { steps: [], info: {} },\n",
       "          header: window.innerHeight >= 600,\n",
       "          height: window.innerHeight,\n",
       "          interactive: false,\n",
       "          legend: true,\n",
       "          loading: false,\n",
       "          playing: false,\n",
       "          processing: false,\n",
       "          renderer: () => &quot;DNE&quot;,\n",
       "          settings: false,\n",
       "          speed: speeds[4],\n",
       "          step: 0,\n",
       "          width: window.innerWidth,\n",
       "        });\n",
       "\n",
       "        // Context helpers.\n",
       "        const rerender = (contextRef.current.rerender = () =>\n",
       "          setRenderCount((renderCountRef.current += 1)));\n",
       "        const setStep = (contextRef.current.setStep = (newStep) => {\n",
       "          contextRef.current.step = newStep;\n",
       "          rerender();\n",
       "        });\n",
       "        const setPlaying = (contextRef.current.setPlaying = (playing) => {\n",
       "          contextRef.current.playing = playing;\n",
       "          rerender();\n",
       "        });\n",
       "        const pause = (contextRef.current.pause = () => setPlaying(false));\n",
       "\n",
       "        const playNext = () => {\n",
       "          const context = contextRef.current;\n",
       "\n",
       "          if (\n",
       "            context.playing &&\n",
       "            context.step < context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(context.step + 1);\n",
       "            play(true);\n",
       "          } else {\n",
       "            pause();\n",
       "          }\n",
       "        };\n",
       "\n",
       "        const play = (contextRef.current.play = (continuing) => {\n",
       "          const context = contextRef.current;\n",
       "          if (context.playing && !continuing) return;\n",
       "          if (!context.playing) setPlaying(true);\n",
       "          if (\n",
       "            !continuing &&\n",
       "            context.step === context.environment.steps.length - 1\n",
       "          ) {\n",
       "            setStep(0);\n",
       "          }\n",
       "          setTimeout(playNext, context.speed);\n",
       "        });\n",
       "\n",
       "        const updateContext = (o) => {\n",
       "          const context = contextRef.current;\n",
       "          Object.assign(context, o, {\n",
       "            environment: { ...context.environment, ...(o.environment || {}) },\n",
       "          });\n",
       "          rerender();\n",
       "        };\n",
       "\n",
       "        // First time setup.\n",
       "        useEffect(() => {\n",
       "          // Timeout is used to ensure useEffect renders once.\n",
       "          setTimeout(() => {\n",
       "            // Initialize context with window.kaggle.\n",
       "            updateContext(window.kaggle || {});\n",
       "\n",
       "            if (window.kaggle.playing) {\n",
       "                play(true);\n",
       "            }\n",
       "\n",
       "            // Listen for messages received to update the context.\n",
       "            window.addEventListener(\n",
       "              &quot;message&quot;,\n",
       "              (event) => {\n",
       "                // Ensure the environment names match before updating.\n",
       "                try {\n",
       "                  if (\n",
       "                    event.data.environment.name ==\n",
       "                    contextRef.current.environment.name\n",
       "                  ) {\n",
       "                    updateContext(event.data);\n",
       "                  }\n",
       "                } catch {}\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "            // Listen for keyboard commands.\n",
       "            window.addEventListener(\n",
       "              &quot;keydown&quot;,\n",
       "              (event) => {\n",
       "                const {\n",
       "                  interactive,\n",
       "                  isInteractive,\n",
       "                  playing,\n",
       "                  step,\n",
       "                  environment,\n",
       "                } = contextRef.current;\n",
       "                const key = event.keyCode;\n",
       "                const zero_key = 48\n",
       "                const nine_key = 57\n",
       "                if (\n",
       "                  interactive ||\n",
       "                  isInteractive() ||\n",
       "                  (key !== 32 && key !== 37 && key !== 39 && !(key >= zero_key && key <= nine_key))\n",
       "                )\n",
       "                  return;\n",
       "\n",
       "                if (key === 32) {\n",
       "                  playing ? pause() : play();\n",
       "                } else if (key === 39) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step < environment.steps.length - 1) setStep(step + 1);\n",
       "                  rerender();\n",
       "                } else if (key === 37) {\n",
       "                  contextRef.current.playing = false;\n",
       "                  if (step > 0) setStep(step - 1);\n",
       "                  rerender();\n",
       "                } else if (key >= zero_key && key <= nine_key) {\n",
       "                  contextRef.current.speed = speeds[key - zero_key];\n",
       "                }\n",
       "                event.preventDefault();\n",
       "                return false;\n",
       "              },\n",
       "              false\n",
       "            );\n",
       "          }, 1);\n",
       "        }, []);\n",
       "\n",
       "        if (contextRef.current.debug) {\n",
       "          console.log(&quot;context&quot;, contextRef.current);\n",
       "        }\n",
       "\n",
       "        // Ability to update context.\n",
       "        contextRef.current.update = updateContext;\n",
       "\n",
       "        // Ability to communicate with ipython.\n",
       "        const execute = (contextRef.current.execute = (source) =>\n",
       "          new Promise((resolve, reject) => {\n",
       "            try {\n",
       "              window.parent.IPython.notebook.kernel.execute(source, {\n",
       "                iopub: {\n",
       "                  output: (resp) => {\n",
       "                    const type = resp.msg_type;\n",
       "                    if (type === &quot;stream&quot;) return resolve(resp.content.text);\n",
       "                    if (type === &quot;error&quot;) return reject(new Error(resp.evalue));\n",
       "                    return reject(new Error(&quot;Unknown message type: &quot; + type));\n",
       "                  },\n",
       "                },\n",
       "              });\n",
       "            } catch (e) {\n",
       "              reject(new Error(&quot;IPython Unavailable: &quot; + e));\n",
       "            }\n",
       "          }));\n",
       "\n",
       "        // Ability to return an action from an interactive session.\n",
       "        contextRef.current.act = (action) => {\n",
       "          const id = contextRef.current.environment.id;\n",
       "          updateContext({ processing: true });\n",
       "          execute(`\n",
       "            import json\n",
       "            from kaggle_environments import interactives\n",
       "            if &quot;${id}&quot; in interactives:\n",
       "                action = json.loads('${JSON.stringify(action)}')\n",
       "                env, trainer = interactives[&quot;${id}&quot;]\n",
       "                trainer.step(action)\n",
       "                print(json.dumps(env.steps))`)\n",
       "            .then((resp) => {\n",
       "              try {\n",
       "                updateContext({\n",
       "                  processing: false,\n",
       "                  environment: { steps: JSON.parse(resp) },\n",
       "                });\n",
       "                play();\n",
       "              } catch (e) {\n",
       "                updateContext({ processing: resp.split(&quot;\\n&quot;)[0] });\n",
       "                console.error(resp, e);\n",
       "              }\n",
       "            })\n",
       "            .catch((e) => console.error(e));\n",
       "        };\n",
       "\n",
       "        // Check if currently interactive.\n",
       "        contextRef.current.isInteractive = () => {\n",
       "          const context = contextRef.current;\n",
       "          const steps = context.environment.steps;\n",
       "          return (\n",
       "            context.interactive &&\n",
       "            !context.processing &&\n",
       "            context.step === steps.length - 1 &&\n",
       "            steps[context.step].some((s) => s.status === &quot;ACTIVE&quot;)\n",
       "          );\n",
       "        };\n",
       "\n",
       "        return h`\n",
       "          <${Context.Provider} value=${contextRef.current}>\n",
       "            <${Player} />\n",
       "          <//>`;\n",
       "      };\n",
       "\n",
       "      preact.render(h`<${App} />`, document.body);\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n",
       "\" width=\"500\" height=\"450\" frameborder=\"0\"></iframe> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "# Play as the first agent against default \"random\" agent.\n",
    "env.run([PPOAgent, \"random\"])\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e9c4f0-2e22-484e-845e-63d6afc63343",
   "metadata": {},
   "source": [
    "## Training PPO Agent within the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69bd42aa-2acf-45b5-9f3d-d3e22e676772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Action 5\n",
      "My Action 6\n",
      "My Action 6\n",
      "My Action 0\n",
      "My Action 3\n",
      "My Action 4\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 1 | 2 | 2 | 0 | 0 | 0 | 1 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2 | 2 | 2 | 1 | 1 | 1 | 1 |\n",
      "+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Play as first position against random agent.\n",
    "trainer = env.train([None, \"random\"])\n",
    "\n",
    "observation = trainer.reset()\n",
    "\n",
    "while not env.done:\n",
    "    my_action = PPOAgent(observation, env.configuration)\n",
    "    print(\"My Action\", my_action)\n",
    "    observation, reward, done, info = trainer.step(my_action)\n",
    "    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\n",
    "print(env.render(mode=\"ansi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f494236-7270-4db2-9e26-4e1ecd2e7f98",
   "metadata": {},
   "source": [
    "#### Evaluate your Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c795ad6-d7ee-4b20-8c25-e35cda509907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Agent vs Random Agent: 0.6\n",
      "My Agent vs Negamax Agent: 0.6\n"
     ]
    }
   ],
   "source": [
    "def mean_reward(rewards):\n",
    "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
    "\n",
    "# Run multiple episodes to estimate its performance.\n",
    "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [PPOAgent, \"random\"], num_episodes=10)))\n",
    "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [PPOAgent, \"random\"], num_episodes=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e08a94d-ba77-44ca-8a19-fb56efed73f7",
   "metadata": {},
   "source": [
    "## Summary Report "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf8842-afe4-4c99-b08c-dfa30a6c8363",
   "metadata": {},
   "source": [
    "\n",
    "### Improved Strategy Learning:\n",
    "\n",
    "- PPO's policy-gradient-based approach allows it to learn more robust and dynamic strategies, which outperformed the Q-learning-based DQN and Double DQN agents.\n",
    "\n",
    "- The continuous updates and clipping in PPO ensure stable policy improvements, reducing overfitting to specific scenarios.\n",
    "\n",
    "### Performance Against Random Agent:\n",
    "\n",
    "- A positive average reward against the random agent shows the PPO agent has learned effective gameplay strategies, consistently outperforming a naive opponent.\n",
    "\n",
    "### Performance Against Negamax Agent:\n",
    "\n",
    "- Achieving a comparable average reward against the Negamax agent (a heuristic-based opponent) demonstrates that the PPO agent has developed strategic decision-making capabilities.\n",
    "\n",
    "### Comparison with DQN and Double DQN:\n",
    "\n",
    "- DQN and Double DQN agents generally rely on value-based methods, which may struggle with complex state-action spaces like ConnectX.\n",
    "PPOs policy-gradient methods provide an advantage by directly optimizing actions based on expected rewards.\n",
    "\n",
    "\n",
    "### Conclusion:\n",
    "- The Enhanced PPO Agent demonstrates superior performance in ConnectX, outperforming Q-learning-based methods and showcasing its potential for solving complex, sequential decision-making problems. It is a promising candidate for further optimization and deployment in competitive ConnectX environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb58b4-9920-47b4-9cf7-dadb69f708b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (connectx_env)",
   "language": "python",
   "name": "connectx_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
